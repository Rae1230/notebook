---
sort: 1
---

# Introduction

<figure>
   <img src="./images/1/1.JPG" width=400px>
</figure>

&emsp;&emsp;本课程将聚集**深度学习**，如果说机器学习=寻找函数，那么深度学习中机器就是要找一个函数，而这个函数是一个类神经网络。函数的输入可以有很多种类，向量、矩阵、序列等。

<figure>
   <img src="./images/1/2.JPG" width=550px>
</figure>

## 1.1 Training

<figure>
   <img src="./images/1/4.JPG" width=500px>
</figure>

**1. Function with unknown parameters**

$$y = f(\qquad) \rightarrow y = b + w x_1$$

&emsp;&emsp;根据domain knowledge将$$y = f(\quad)$$写出<b><font color="#00B050">model</font></b> $$y = b + w x_1$$，其中，$$y$$和$$x_1$$为<b><font color="#00B050">feature</font></b>，$$w$$和$$b$$为未知参数，称$$w$$为<b><font color="#00B050">weight</font></b>，称$$b$$为<b><font color="#00B050">bias</font></b>。

**2. Define loss from training data**

&emsp;&emsp;<b><font color="#00B050">Loss</font></b>是关于参数的函数$$L(b,w)$$，用于评估一组值的好坏程度。

令loss：$$L = \frac{1}{N} \sum_n e_n$$  
&emsp;&emsp;若$$e = \| y - \hat{y} \|$$，则$$L$$为mean absolute error (MAE)  
&emsp;&emsp;若$$e = (y - \hat{y})^2$$，则$$L$$为mean square error (MSE)。

如果$$y$$和$$\hat{y}$$都是概率分布，可以选择cross entropy（交叉熵）作为loss。

**3. Optimization**

Gradient descent  $$w^*, b^* = \arg \min_{w, b} L$$
* （随机）选取一个初始值$$w^0$$，$$b^0$$
* 计算$$\frac{\partial L}{\partial w} \vert_{w = w_0, b = b_0}$$和$$\frac{\partial L}{\partial b} \vert_{w = w_0, b = b_0}$$  
  通过选取合适的learning rate $$\color{green}{\eta}$$来控制更新的步长。因为$$\eta$$是人为选定的，所以是一个hyperparameter。  
  &emsp;&emsp;$$w^1 \leftarrow w^0 - \eta \frac{\partial L}{\partial w} \vert_{w = w_0, b = b_0} \qquad b^1 \leftarrow b^0 - \eta \frac{\partial L}{\partial b} \vert_{w = w_0, b = b_0}$$
* 迭代更新$$w$$和$$b$$

```warning
梯度下降法可能会陷入局部极值，但这并不是在这个方法中所要解决的真正难题。
```

&emsp;&emsp;前面我们考虑了线性模型$$y = b + w x_1$$，但是线性模型非常受限，即有<b><font color="#00B050">model bias</font></b>，所以还需要更加复杂的模型。




<!-- 蓝 -->
<b><font color="#3399ff"></font></b>
<!-- 绿 --><!-- #33cc00 -->
<b><font color="#00B050"></font></b>
<!-- 橙 -->
<b><font color="#FF4500"></font></b>