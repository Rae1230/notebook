---
sort: 1
---

# Overview

#### 强化学习的特征

* 探索过程
* 奖励延迟
* 时间有影响（序列数据，而非i.i.d.（独立同分布）的）
* 智能体的行为会影响随后得到的数据（agent的行为会改变环境）

```note
强化学习可以取得超人的效果。
```

## 序列决策简介

### Rewards

* 奖励是标量反馈信号
* 表明agent在第t步表现如何
* 强化学习是基于最大化奖励的：agent的目标可以描述为使期望积累的奖励最大。

### Sequential decision making

* agent的目标：选择一系列行为来最大化未来总奖励
* 行为可能会产生长期影响
* 奖励可能会延迟
* 短期奖励与长期奖励的权衡
* 历史是观察、行为、奖励的序列$$H_t = O_1, R_1, A_1, \ldots, A_{t-1}, O_t, R_t$$
* 下面会发生什么与历史有关
* 状态是用于决定下面发生什么的函数$$S_t = f(H_t)$$
* 环境状态$$S_t^e = f^e(H_t)$$和agent状态$$S_t^a = f^a(H_t)$$
* <b><font color="#00B050">完全可观性</font></b>：agent可以直接观察到环境状态，被建模为马尔可夫决策过程（MDP）$$O_t = S_t^e = S_t^a$$
* <b><font color="#00B050">部分可观性</font></b>：agent无法直接观察到环境状态，被建模为部分可观马尔可夫决策过程（POMDP）

### RL angent的主要成分

一个RL angent可能包括以下一个或多个成分：  
* 策略：agent的行为函数
* 价值函数：评估当前状态或行为
* 模型：agent对环境的理解

#### Policy

* 策略是agent的行为模型
* 是从输入状态/观察到行为的映射
  
有两种策略：  
1. Stochastic policy（随机策略）：概率样本$$\pi (a \vert s) = P[A_t = a \vert S_t = s]$$
2. Deterministic policy（确定性策略）：$$a^* = \mathop{argmax}_a \pi (a \vert s)$$




<br />
<!-- 蓝 -->
<font color="#3399ff"></font>
<!-- 绿 --><!-- #33cc00 -->
<b><font color="#00B050"></font></b>
<!-- 橙 -->
<font color="#FF4500"></font>