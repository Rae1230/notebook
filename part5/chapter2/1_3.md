---
sort: 1.3
---

# Brief Talk on ML

## The Pokémon Digimon Classifier

<figure><img src="./images/1/49.JPG" width=630px></figure>

&emsp;&emsp;以Pokémon vs. Digimon作为案例。我们希望找到一个函数，可以实现

<figure><img src="./images/1/50.JPG" width=230px></figure>

即确定一个带有未知参数的函数（基于领域知识）。

**Observation**

<figure><img src="./images/1/51.JPG" width=550px></figure>

**Function with Unknown Parameters**

<figure><img src="./images/1/52.JPG" width=450px></figure>

**Loss of a function** (given data)

<figure><img src="./images/1/53.JPG" width=620px></figure>

### Training Examples

<figure><img src="./images/1/54.JPG" width=580px></figure>

<figure><img src="./images/1/55.JPG" width=600px></figure>

<figure><img src="./images/1/56.JPG" width=600px></figure>

&emsp;&emsp;$$L(\color{red}{h^{all}, D_{all}} \color{black}{)}$$只有在$$D_{all}$$上是最小的，所以$$L(\color{blue}{h^{train}, D_{train}} \color{black}{)}$$是可以比$$L(\color{red}{h^{all}, D_{all}} \color{black}{)}$$小的。

### What do we want?

<figure><img src="./images/1/57.JPG" width=630px></figure>

所以，我们希望采样到**good** $$\color{blue}{D_{train}}$$满足

<figure><img src="./images/1/58.JPG" width=350px></figure>

那么到底有多大概率会采样到**bad** $$\color{blue}{D_{train}}$$呢？

### Probability of failure

```note
以下讨论very general，是model-agnostic的，无需关于data distribution的假设，可以用任意的loss function。
```

<figure><img src="./images/1/59.JPG" width=530px></figure>

<figure><img src="./images/1/60.JPG" width=630px></figure>

<figure><img src="./images/1/61.JPG" width=630px></figure>

<figure><img src="./images/1/62.JPG" width=630px></figure>

**例**：<figure><img src="./images/1/63.JPG" width=620px></figure>

### Model Complexity

<figure><img src="./images/1/64.JPG" width=500px></figure>

如果参数是连续的呢？  
&emsp;&emsp;Answer 1: 计算机中所有计算都是离散的。  
&emsp;&emsp;Answer 2: VC-dimension (not this course)

<figure><img src="./images/1/65.JPG" width=600px></figure>

#### Tradeoff of Model Complexity

<figure><img src="./images/1/66.JPG" width=600px></figure>


## Why Deep?

### Review: Why hidden layer?

#### Piecewise linear

<figure><img src="./images/1/67.JPG" width=580px></figure>

Hard Sigmoid → Sigmoid function
<figure><img src="./images/1/68.JPG" width=650px></figure>

Hard Sigmoid → ReLU

<figure><img src="./images/1/69.JPG" width=650px></figure>

```tip
为什么我们想要"Deep" network，而不是"Fat" network？
```

### Deep is better?

<figure><img src="./images/1/70.JPG" width=420px></figure>

<figure><img src="./images/1/71.JPG" width=330px></figure>

<figure>
    <img src="./images/1/72.JPG" width=450px>
    <figcaption>Seide Frank, Gang Li, and Dong Yu. "Conversational Speech ranscription Using Context-Dependent Deep Neural Networks." Interspeech. 2011.
    <figcaption>
</figure>

### Why we need deep?

<figure><img src="./images/1/73.JPG" width=600px></figure>

**Analogy – Logic Circuits**

例，parity check

(to be continued...)




<!-- 蓝 -->
<b><font color="#3399ff"></font></b>
<!-- 绿 --><!-- #33cc00 -->
<b><font color="#00B050"></font></b>
<!-- 橙 -->
<b><font color="#FF4500"></font></b>