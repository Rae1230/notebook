---
sort: 2
---

# What to Do If Training Fails

## 2.1 When Gradient is Small

<figure><img src="./images/2/1.JPG" width=550px></figure>

优化为什么会失败呢？  
— 梯度为零，有两种情况：局部极小值和鞍点，这两种点统称为**critical point**（驻点）。在局部极小值点无路可走，而鞍点只要能够逃离的话还是可以继续下降的。

&emsp;&emsp;如何判断是局部极小还是鞍点呢？

### 2.1.0 Local minima & Saddle point

#### Taylor series approximation

<figure><img src="./images/2/2.JPG" width=550px></figure>

#### Hessian

<figure><img src="./images/2/3.JPG" width=550px></figure>

<figure><img src="./images/2/4.JPG" width=550px></figure>

**例**：<figure><img src="./images/2/5.JPG" width=650px></figure>

#### Don't afraid of saddle point?

&emsp;&emsp;$$H$$或许可以告诉我们参数更新的方向！

<figure><img src="./images/2/6.JPG" width=450px></figure>

**例**：<figure><img src="./images/2/7.JPG" width=580px></figure>

### 2.1.1 Saddle point v.s. Local minima

<figure><img src="./images/2/8.JPG" width=600px></figure>

&emsp;&emsp;低维空间里的局部极小值点可能只是高维空间中的一个鞍点，那么当你有很多参数的时候，会不会局部极小值点就很少见了呢？

<figure><img src="./images/2/9.JPG" width=700px></figure>

### 2.1.2 Small gradient

<figure><img src="./images/2/10.JPG" width=400px></figure>

## 2.2 Tips: Batch & Momentum

### 2.2.1 Batch

#### Review: Optimization with batch

<figure><img src="./images/2/11.JPG" width=450px></figure>

#### Small batch v.s. Large batch

&emsp;&emsp;考虑有20个样本的情况（$$N=20$$）：

<figure><img src="./images/2/12.JPG" width=550px></figure>

左边的方法更新的一步更稳定，但是看起来花的时间长。但是实际上，考虑到并行运算，<b><font color="#3399ff">大的batch size在计算梯度的时候并不需要更长的时间</font></b>。

<figure>
    <img src="./images/2/13.JPG" width=450px>
    <figcaption>用Tesla V100 GPU在MNIST数据集上每次更新的时间随batch size变化的情况</figcaption>
</figure>

&emsp;&emsp;并且，<b><font color="#3399ff">较小的batch一次epoch所需的时间反而更长</font></b>（花更多的时间来把所有的数据看一遍）。

<figure><img src="./images/2/14.JPG" width=450px></figure>

&emsp;&emsp;那么大的batch就一定好吗？

<figure>
    <img src="./images/2/15.JPG" width=450px>
    <figcaption>不同训练集上准确率随batch size变化的情况</figcaption>
</figure>

从上图可以看出，小的batch size的表现更好。那么大的batch size有什么问题呢？  — <b><font color="#FF4500">Optimization Fails</font></b>（因为用的都是一样的模型，所以不是因为model bias）

* <b><font color="#3399ff">小的batch size性能更好</font></b>
* <b><font color="#3399ff">“noisy”的更新有利于训练</font></b>

<figure><img src="./images/2/16.JPG" width=350px></figure>

此外，<b><font color="#3399ff">small batch在测试数据上表现也更好</font></b>。

<figure>
    <img src="./images/2/17.JPG" width=550px>
    <figcaption>On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima中的结果</figcaption>
</figure>

<figure><img src="./images/2/18.JPG" width=400px></figure>

&emsp;&emsp;Small Batch v.s. Large Batch，两者各有优缺点，所以**batch size是一个需要我们决定的hyperparameter**。

<figure><img src="./images/2/19.JPG" width=420px></figure>

两者兼得？  
&emsp;&emsp;[Large Batch Optimization for Deep Learning: Training BERT in 76 minutes](https://arxiv.org/abs/1904.00962)  
&emsp;&emsp;[Extremely Large Minibatch SGD: Training ResNet-50 on ImageNet in 15 Minutes](https://arxiv.org/abs/1711.04325)  
&emsp;&emsp;[Stochastic Weight Averaging in Parallel: Large-Batch Training That Generalizes Well](https://arxiv.org/abs/2001.02312)  
&emsp;&emsp;[Large Batch Training of Convolutional Networks](https://arxiv.org/abs/1708.03888)  
&emsp;&emsp;[Accurate, large minibatch sgd: Training imagenet in 1 hour](https://arxiv.org/abs/1706.02677)

### 2.2.2 Momentum

<figure><img src="./images/2/20.JPG" width=350px></figure>

&emsp;&emsp;考虑物理世界，error surface就是斜坡，而参数是一个球，球从斜坡上滚下来，由于惯性的存在，它不一定会被鞍点或局部极小点卡住。

#### (Vanilla) Gradient descent

<figure><img src="./images/2/21.JPG" width=400px></figure>

&emsp;&emsp;一般的梯度下降：计算初始点处的梯度，然后往梯度的反方向更新一次参数，得到一组新的参数，然后计算梯度并往梯度的反方向更新一次参数……不断重复这个过程。

#### Gradient descent + Momentum

<figure><img src="./images/2/23.JPG" width=620px></figure>

<figure><img src="./images/2/24.JPG" width=450px></figure>

### 2.2.3 Concluding remarks

* 驻点处梯度为零
* 驻点可能是鞍点或局部极小值点
  * 可以利用Hessian矩阵来区别
  * 沿着Hessian矩阵的特征向量可以逃离鞍点
  * 局部极小点可能是很少见的
* 更小的batch size和momentum可以帮助逃离驻点

## 2.3 Tip: Adaptive Learning Rate

### 2.3.1 Training stuck ≠ Small gradient 

<figure><img src="./images/2/25.JPG" width=550px></figure>

&emsp;&emsp;人们总是以为训练卡住是因为参数到了驻点附近，但是实际计算一下梯度大小的变化，可以发现其实梯度并没有真的变得很小。

&emsp;&emsp;如果说训练停住往往不是因为驻点，那是因为什么呢？以一个凸的error surface为例，从头到尾采用同一个学习率是得不到好的结果的。

<figure><img src="./images/2/26.JPG" width=600px></figure>

### 2.3.2 Adaptive learning rate

<figure><img src="./images/2/27.JPG" width=300px></figure>

&emsp;&emsp;不同的参数需要不同的学习率。如果在某个方向上梯度的值很小，梯度非常平坦，那么我们会希望学习率大一点；反之，如果在某个方向上非常陡峭，那么会希望学习率小一点。那么如何来自适应地调整学习率呢？

&emsp;&emsp;以某一个参数$$\pmb{\theta}$$更新的过程为例。原来$$\pmb{\theta}$$的更新为 $$\pmb{\theta}_i^{t+1} \leftarrow \pmb{\theta}_i^t - \eta g_i^t$$，$$\boldsymbol{g}_i^t = \left. \frac{\partial L}{\partial \pmb{\theta}_i} \right\vert_{\pmb{\theta} = \pmb{\theta}^{t}}$$。现在，将$$\eta$$这项改为$$\frac{\eta}{\sigma_i^t}$$，这是一个与参数相关的学习率。

<figure><img src="./images/2/28.JPG" width=300px></figure>

#### Root mean square

<figure>
    <img src="./images/2/29.JPG" width=600px>
    <figcaption>AdaGrad中采用的方法</figcaption>
</figure>

&emsp;&emsp;在AdaGrade中，即使是同样的参数，所需的学习率也会随着时间的改变而变化。

#### Learning rate adapts dynamically

<figure><img src="./images/2/30.JPG" width=350px></figure>

&emsp;&emsp;之前，我们似乎假设同一个参数对应的梯度都是差不多的值。实际上，以上图为例，同一个参数我们也期望有不同的学习率，那就需要学习率能够动态地适应调整。

#### RMSProp

<figure><img src="./images/2/32.JPG" width=550px></figure>

#### Adam: RMSProp + Momentum 

<figure><img src="./images/2/31.JPG" width=600px></figure>

原文：[ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION](https://arxiv.org/pdf/1412.6980.pdf)

### 2.3.3 Learning rate scheduling 

<figure>
    <img src="./images/2/33.JPG" width=300px>
    <figcaption>用AdaGrad的方法来调整学习率的结果</figcaption>
</figure>

&emsp;&emsp;如何解决这种震荡问题呢？

<figure><img src="./images/2/34.JPG" width=120px></figure>

<figure><img src="./images/2/35.JPG" width=500px></figure>

### 2.3.4 Summary of optimization

<figure><img src="./images/2/36.JPG" width=550px></figure>

## 2.4 Loss Function: Classification


<!-- 蓝 -->
<b><font color="#3399ff"></font></b>
<!-- 绿 --><!-- #33cc00 -->
<b><font color="#00B050"></font></b>
<!-- 橙 -->
<b><font color="#FF4500"></font></b>