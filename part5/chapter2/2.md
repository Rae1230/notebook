---
sort: 2
---

# Markov Decision Process (MDP)

<center>
    <figure>
        <img src="./images/2-1.JPG" width=300px>
    </figure>
</center>

马尔科夫决策过程可以模拟许多现实世界的问题，它正式描述了强化学习的框架。

在MDP下，环境是完全可观的。  
* 最优控制主要处理连续的MDP
* 部分可观问题可以转化为MDP

---

Markov Property：
1. 历史状态 $$h_t = \{ s_1, s_2, \ldots, s_t \}$$
2. 状态$$s_t$$是马尔可夫的，当且仅当
$$ p(s_{t+1} \vert s_t) = p(s_{t+1} \vert h_t) \tag{1}$$
$$ p(s_{t+1} \vert s_t, a_t) = p(s_{t+1} \vert h_t, a_t) \tag{2}$$
3. “The future is independent of the past given the present.”

## 2.1 Markov Processes/ Chain

<center>
    <figure>
        <img src="./images/2-2.JPG" width=180px>
    </figure>
</center>

状态转移矩阵$$P$$指定了$$p(s_{t+1} = s' \vert s_t = s)$$

$$  P = \begin{bmatrix}
    P(s_1 \vert s_1) & P(s_2 \vert s_1) & \cdots & P(s_N \vert s_1) \\
    P(s_1 \vert s_2) & P(s_2 \vert s_2) & \cdots & P(s_N \vert s_2) \\
    \vdots           & \vdots           & \ddots & \vdots           \\
    P(s_1 \vert s_N) & P(s_2 \vert s_N) & \cdots & P(s_N \vert s_N)
\end{bmatrix}
$$

## 2.2 Markov Reward Processes (MRPs)

马尔可夫奖励过程为 马尔可夫链 + 奖励

<b><font color="#00B050">Markov Reward Processes</font></b>：  
&emsp;1. $$S$$是状态（$$s \in S$$）的（有限）结合  
&emsp;2. $$P$$是指定$$p(s_{t+1} = s' \vert s_t = s)$$的动力学/过渡模型  
&emsp;3. $$R$$是<b><font color="#00B050">reward function</font></b> $$R(s_t = s) = \mathbb{E}[r_t \vert s_t = s]$$  
&emsp;4. Discount factor $$\gamma \in [0,1]$$

如果状态数有限，$$R$$可以是一个向量。

#### Return and Value function

<b><font color="#00B050">Horizon</font></b>：  
* 每集的最大时间步数 2 
* 可以是无限的，否则称为有限马尔可夫（奖励）过程

<b><font color="#00B050">Return</font></b>：  
* 时间步数$$t$$到horizon的奖励的折扣总和
$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots + \gamma^{T-t-1} R_{T}$$

MRP的状态值函数$$V_t(s)$$：
* 时刻$$t$$状态$$s$$下的return期望值
$$\begin{aligned}
    V_t(s)  &= \mathbb{E}[G_t \vert s_t = s] \\
            &= \mathbb{E}[
                    R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots + \gamma^{T-t-1} R_{T}
                    \vert s_t = s]
\end{aligned}$$
* 未来奖励的当前价值

#### Why Discount Factor?

1. 避免循环马尔可夫过程中的无限回报
2. 未来的不确定性可能无法充分体现
3. 如果奖励是有实际价值的，即时奖励可能比延迟奖励更有吸引力
4. 动物/人类的行为表现出对即时奖励的偏好
5. 有时可以使用没有折扣的马尔可夫奖励过程（即$$\gamma=1$$），e.g. 如果所有序列终止。 

&emsp;&emsp;$$\gamma = 0$$：只关心即时奖励  
&emsp;&emsp;$$\gamma = 1$$：延迟奖励等于即时奖励

#### MRP值函数的计算

值函数：从状态$$s$$开始的期望return
$$  V(s) 
=   \mathbb{E}[G_t \vert s_t = s]
=   \mathbb{E}[
        R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots 
        \vert s_t = s]
$$

MRP值函数满足如下的**Bellman equation**：
$$  V(s)
=   \underbrace{R(s)}
  + \underbrace{\gamma \sum_{s' \in S}{P(s/ \vert s) V(s')}}
$$

其中，$$R(s)$$为即时奖励，$$\gamma \sum_{s' \in S}{P(s/ \vert s) V(s')}$$为延迟奖励的折扣总和。

&emsp;&emsp;Bellman equation描述了状态间的迭代关系。

<center>
    <figure>
        <img src="./images/2-3.JPG" width=400px>
    </figure>
</center>

因此，我们可以把$$V(s)$$写成矩阵形式。
$$  \begin{bmatrix}
        V(s_1) \\ V(s_2) \\ \vdots \\ V(s_N)
    \end{bmatrix}
=   \begin{bmatrix}
        R(s_1) \\ R(s_2) \\ \vdots \\ R(s_N)
    \end{bmatrix}
  + \gamma \begin{bmatrix}
        P(s_1 \vert s_1) & P(s_2 \vert s_1) & \cdots & P(s_N \vert s_1)\\
        P(s_1 \vert s_2) & P(s_2 \vert s_2) & \cdots & P(s_N \vert s_2) \\
        \vdots           & \vdots           & \ddots & \vdots           \\
        P(s_1 \vert s_N) & P(s_2 \vert s_N) & \cdots & P(s_N \vert s_N)
    \end{bmatrix}
    \begin{bmatrix}
        V(s_1) \\ V(s_2) \\ \vdots \\ V(s_N)
    \end{bmatrix}
$$
$$ V = R + \gamma P V $$

MRP的解析解为$$V = (I - \gamma P)^{-1} R$$
* 对于$$N$$中状态，矩阵求逆的复杂度为$$O(N^3)$$
* 此方法只能解决小型的MRPs

#### MRP值的迭代算法

1. 动态规划
<center>
    <figure>
        <img src="./images/2-5.JPG" width=400px>
    </figure>
</center>

2. Monte-Carlo evaluation
<center>
    <figure>
        <img src="./images/2-4.JPG" width=400px>
    </figure>
</center>

3. Temporal-Difference learning

## 2.3 Markov Decision Processes (MDPs)

马尔可夫决策过程为 马尔可夫奖励过程 + 决策

<b><font color="#00B050">Markov Decision Processes</font></b>：  
&emsp;1. $$S$$是状态（$$s \in S$$）的（有限）结合  
&emsp;2. $$\color{green}{A}$$是<b><font color="#00B050">行为的有限集合</font></b>  
&emsp;3. $$P^a$$是各个行为对应的动力学/过渡模型$$p(s_{t+1} = s' \vert s_t = s ,\ \color{green}{a_t = a})$$  
&emsp;4. $$R$$是<b><font color="#00B050">reward function</font></b> $$R(s_t = s ,\ a_t = a) = \mathbb{E}[r_t \vert s_t = s ,\ \color{green}{a_t = a}]$$  
&emsp;5. Discount factor $$\gamma \in [0,1]$$

MDP是一个元组：$$(S ,\ A ,\ P ,\ R ,\ \gamma)$$





<br />
<!-- 蓝 -->
<font color="#3399ff"></font>
<!-- 绿 --><!-- #33cc00 -->
<b><font color="#00B050"></font></b>
<!-- 橙 -->
<font color="#FF4500"></font>