---
sort: 2
---

# Markov Decision Process (MDP)

<center>
    <figure>
        <img src="./images/2-1.JPG" width=300px>
    </figure>
</center>

马尔科夫决策过程可以模拟许多现实世界的问题，它正式描述了强化学习的框架。

在MDP下，环境是完全可观的。  
* 最优控制主要处理连续的MDP
* 部分可观问题可以转化为MDP

---

Markov Property：  
&emsp;1. 历史状态 $$h_t = \{ s_1, s_2, \ldots, s_t \}$$  
&emsp;2. 状态$$s_t$$是马尔可夫的，当且仅当  
   
$$ p(s_{t+1} \vert s_t) = p(s_{t+1} \vert h_t) \tag{1}$$
$$ p(s_{t+1} \vert s_t, a_t) = p(s_{t+1} \vert h_t, a_t) \tag{2}$$

&emsp;3. “The future is independent of the past given the present.”

## 2.1 Markov Processes/ Chain

<center>
    <figure>
        <img src="./images/2-2.JPG" width=180px>
    </figure>
</center>

状态转移矩阵$$P$$指定了$$p(s_{t+1} = s' \vert s_t = s)$$

$$  P = \begin{bmatrix}
    P(s_1 \vert s_1) & P(s_2 \vert s_1) & \cdots & P(s_N \vert s_1) \\
    P(s_1 \vert s_2) & P(s_2 \vert s_2) & \cdots & P(s_N \vert s_2) \\
    \vdots           & \vdots           & \ddots & \vdots           \\
    P(s_1 \vert s_N) & P(s_2 \vert s_N) & \cdots & P(s_N \vert s_N)
\end{bmatrix}
$$

## 2.2 Markov Reward Processes (MRPs)

马尔可夫奖励过程为 **马尔可夫链 + 奖励**

<b><font color="#00B050">Markov Reward Processes</font></b>：  
&emsp;1. $$S$$是状态（$$s \in S$$）的（有限）结合  
&emsp;2. $$P$$是指定$$p(s_{t+1} = s' \vert s_t = s)$$的动力学/过渡模型  
&emsp;3. $$R$$是<b><font color="#00B050">reward function</font></b> $$R(s_t = s) = \mathbb{E}[r_t \vert s_t = s]$$  
&emsp;4. Discount factor $$\gamma \in [0,1]$$

如果状态数有限，$$R$$可以是一个向量。

### 2.2.1 Return and Value function

<b><font color="#00B050">Horizon</font></b>：  
* 每集的最大时间步数 2 
* 可以是无限的，否则称为有限马尔可夫（奖励）过程

<b><font color="#00B050">Return</font></b>：  
* 时间步数$$t$$到horizon的奖励的折扣总和
$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots + \gamma^{T-t-1} R_{T}$$

MRP的状态值函数 $$V_t(s) = \mathbb{E}[G_t \vert s_t = s] = \mathbb{E}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots + \gamma^{T-t-1} R_{T}\vert s_t = s]$$
* 时刻$$t$$状态$$s$$下的return期望值
* 未来奖励的当前价值

### 2.2.2 Why Discount Factor?

1. 避免循环马尔可夫过程中的无限回报
2. 未来的不确定性可能无法充分体现
3. 如果奖励是有实际价值的，即时奖励可能比延迟奖励更有吸引力
4. 动物/人类的行为表现出对即时奖励的偏好
5. 有时可以使用没有折扣的马尔可夫奖励过程（即$$\gamma=1$$），e.g. 如果所有序列终止。  
&emsp;&emsp;$$\gamma = 0$$：只关心即时奖励  
&emsp;&emsp;$$\gamma = 1$$：延迟奖励等于即时奖励

### 2.2.3 MRP值函数的计算

值函数：从状态$$s$$开始的期望return
$$  V(s) 
=   \mathbb{E}[G_t \vert s_t = s]
=   \mathbb{E}[
        R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots 
        \vert s_t = s]
$$

MRP值函数满足如下的**Bellman equation**：  
$$V(s) = R(s) + \gamma \sum_{s' \in S}{P(s' \vert s) V(s')}$$

其中，$$R(s)$$为即时奖励，$$\gamma \sum_{s' \in S}{P(s' \vert s) V(s')}$$为延迟奖励的折扣总和。

```tip
Bellman equation描述了状态间的迭代关系。
当前状态值函数 = 即时奖励 + 下一步每个状态的值函数×转移概率累计求和之后×折扣因子
```

<center>
    <figure>
        <img src="./images/2-3.JPG" width=420px>
    </figure>
</center>

因此，我们可以把$$V(s)$$写成矩阵形式。

$$  \begin{bmatrix}
        V(s_1) \\ V(s_2) \\ \vdots \\ V(s_N)
    \end{bmatrix}
=   \begin{bmatrix}
        R(s_1) \\ R(s_2) \\ \vdots \\ R(s_N)
    \end{bmatrix}
  + \gamma \begin{bmatrix}
        P(s_1 \vert s_1) & P(s_2 \vert s_1) & \cdots & P(s_N \vert s_1) \\
        P(s_1 \vert s_2) & P(s_2 \vert s_2) & \cdots & P(s_N \vert s_2) \\
        \vdots           & \vdots           & \ddots & \vdots           \\
        P(s_1 \vert s_N) & P(s_2 \vert s_N) & \cdots & P(s_N \vert s_N)
    \end{bmatrix}
    \begin{bmatrix}
        V(s_1) \\ V(s_2) \\ \vdots \\ V(s_N)
    \end{bmatrix}
$$

$$ V = R + \gamma P V $$

MRP的解析解为$$V = (I - \gamma P)^{-1} R$$
* 对于$$N$$中状态，矩阵求逆的复杂度为$$O(N^3)$$
* 此方法只能解决小型的MRPs

### 2.2.4 MRP值的迭代算法

&emsp;1. 动态规划
<center>
    <figure>
        <img src="./images/2-5.JPG" width=450px>
    </figure>
</center>

&emsp;2. Monte-Carlo evaluation
<center>
    <figure>
        <img src="./images/2-4.JPG" width=450px>
    </figure>
</center>

&emsp;3. Temporal-Difference learning

## 2.3 Markov Decision Processes (MDPs)

马尔可夫决策过程为 **马尔可夫奖励过程 + 决策**

<b><font color="#00B050">Markov Decision Processes</font></b>：  
&emsp;1. $$S$$是状态（$$s \in S$$）的（有限）结合  
&emsp;2. $$\color{green}{A}$$是<b><font color="#00B050">行为的有限集合</font></b>  
&emsp;3. $$P^a$$是各个行为对应的动力学/过渡模型 $$p(s_{t+1} = s' \vert s_t = s ,\ \color{green}{a_t = a})$$  
&emsp;4. $$R$$是<b><font color="#00B050">reward function</font></b> $$R(s_t = s ,\ a_t = a) = \mathbb{E}[r_t \vert s_t = s ,\ \color{green}{a_t = a}]$$  
&emsp;5. Discount factor $$\gamma \in [0,1]$$

MDP是一个元组：$$(S ,\ A ,\ P ,\ R ,\ \gamma)$$

### 2.3.1 Policy in MDP

* 策略指定在每个状态下要采取的行动
* 给定一个状态，指定行为的分布情况
* 策略：$$\pi (a \vert s) = P(a_t =a \vert s_t = s)$$
* 策略是静止的（时间独立的），对于任意的$$t \gt 0$$，$$A_t ~ \pi (a \vert s)$$

&emsp;&emsp;给定一个MDP $$(S ,\ A ,\ P ,\ R ,\ \gamma)$$和一个策略$$\pi$$，状态序列$$S_1, S_2, \ldots$$为**马尔可夫过程**$$(S, P^{\pi})$$，状态和奖励序列$$S_1, R_1, S_2, R_2 \ldots$$为**马尔可夫奖励过程**$$(S, P^{\pi}, R^{\pi}, \gamma)$$，其中

$$\begin{aligned}
    P^{\pi}(s' \vert s) &= \sum_{a \in A} \pi(a \vert s) P(s' \vert s, a) \\
    R^{\pi}(s) &= \sum_{a \in A} \pi(a \vert s) R(s, a)
\end{aligned}$$

<center>
    <figure>
        <img src="./images/2-6.JPG" width=400px>
    </figure>
</center>

上图中，左边是MP/MRP的状态转移过程，右边是MDP的状态转移过程，可以看出MDP多了行为决策的过程。

### 2.3.2 Value function for MDP

&emsp;&emsp;MDP的<b><font color="#00B050">状态值函数</font></b>是$$V^{\pi}(s)$$是从状态$$s$$出发，遵循策略$$\pi$$的期望return

$$V^{\pi}(s) = \mathbb{E}_{\pi}[G_t \vert s_t=s]    \tag{3}$$

&emsp;&emsp;<b><font color="#00B050">行为值函数</font></b>$$q^{\pi}(s, a)$$是从状态$$s$$出发，采取行为$$a$$，遵循策略$$\pi$$的期望return

$$q^{\pi}(s, a) = \mathbb{E}_{\pi}[G_t \vert s_t=s, A_t=a]  \tag{4}$$

&emsp;&emsp;$$V^{\pi}(s)$$与$$q^{\pi}(s, a)$$的关系为

$$V^{\pi}(s) = \sum_{a \in A} \pi(a \vert s) q^{\pi}(s, a)  \tag{5}$$

### 2.3.3 Bellman Expectation Equation

&emsp;&emsp;状态值函数可以分解为即时奖励加上后继状态的折扣值

$$  V^{\pi}(s)
=   E_{pi} [R_{t+1} + \gamma V^{\pi}(s_{t+1}) \vert s_t = s]
\tag{6}$$

&emsp;&emsp;行为值函数也可以类似地分解

$$  q^{\pi}(s, a)
=   E_{pi} [   
        R_{t+1} + \gamma q^{\pi}(s_{t+1}, A_{t+1}) 
        \vert s_t = s, A_t = a
    ]
\tag{7}$$

根据

$$V^{\pi}(s) = \sum_{a \in A} \pi (a \vert s) q^{\pi}(s, a) \tag{8}$$

$$  q^{\pi}(s, a) 
=   R_s^a + \gamma \sum_{s' \in S} P(s' \vert s,a) V^{\pi}(s')
\tag{9}$$

因此

$$  V^{\pi}(s) 
=   \sum_{a \in A} \pi(a \vert s) 
        (R(s,a) + \gamma \sum_{s' \in S} P(s' \vert s,a) V^{\pi}(s'))
\tag{10}$$

$$  q^{\pi}(s,a) 
=   R(s,a) + \gamma \sum_{s' \in S} P(s' \vert s,a)
                    \sum_{a' \in A} \pi (a' \vert s') q^{\pi}(s', a')
\tag{11}$$

<center>
    <figure>
        <img src="./images/2-7.JPG" width=500px>
    </figure>
</center>

[Bellman方程的数学证明](https://zhuanlan.zhihu.com/p/108484403)

### 2.3.4 Policy Evaluation

&emsp;&emsp;评估给定策略$$\pi$$下的状态值：计算$$V^{\pi}(s)$$。也称为 (value)  prediction。

<center>
    <figure>
        <img src="./images/2-8.JPG" width=450px>
    </figure>
</center>



<br />
<!-- 蓝 -->
<font color="#3399ff"></font>
<!-- 绿 --><!-- #33cc00 -->
<b><font color="#00B050"></font></b>
<!-- 橙 -->
<font color="#FF4500"></font>