---
sort: 2
---

# Markov Decision Process (MDP)

<center>
    <figure>
        <img src="./images/2-1.JPG" width=240px>
    </figure>
</center>

马尔科夫决策过程可以模拟许多现实世界的问题，它正式描述了强化学习的框架。

在MDP下，环境是完全可观的。  
* 最优控制主要处理连续的MDP
* 部分可观问题可以转化为MDP

---

## 2.1 Markov Model

Markov Property：
1. 历史状态 $$h_t = \{ s_1, s_2, \ldots, s_t \}$$
2. 状态$$s_t$$是马尔可夫的，当且仅当
$$ p(s_{t+1} \vert s_t) = p(s_{t+1} \vert h_t) \tag{1}$$
$$ p(s_{t+1} \vert s_t, a_t) = p(s_{t+1} \vert h_t, a_t) \tag{2}$$
3. “The future is independent of the past given the present.”

### 2.1.1 Markov Processes/ Chain

<center>
    <figure>
        <img src="./images/2-2.JPG" width=180px>
    </figure>
</center>

状态转移矩阵$$P$$指定了$$p(s_{t+1} = s' \vert s_t = s$$

$$  P = \begin{bmatrix}
    P(s_1 \vert s_1) & P(s_2 \vert s_1) & \cdots & P(s_N \vert s_1) \\
    P(s_1 \vert s_2) & P(s_2 \vert s_2) & \cdots & P(s_N \vert s_2) \\


\end{bmatrix}
$$




Markov Reward Processes (MRPs)

Markov Decision Processes (MDPs)

<br />
<!-- 蓝 -->
<font color="#3399ff"></font>
<!-- 绿 --><!-- #33cc00 -->
<b><font color="#00B050"></font></b>
<!-- 橙 -->
<font color="#FF4500"></font>