---
sort: 2
---

# Training Fails

## 2.1 When Gradient is Small

<figure><img src="./images/2/1.JPG" width=550px></figure>

优化为什么会失败呢？  
— 梯度为零，有两种情况：局部极小值和鞍点，这两种点统称为**critical point**（驻点）。在局部极小值点无路可走，而鞍点只要能够逃离的话还是可以继续下降的。

&emsp;&emsp;如何判断是局部极小还是鞍点呢？

### 2.1.0 Local minima & Saddle point

#### Taylor series approximation

<figure><img src="./images/2/2.JPG" width=550px></figure>

#### Hessian

<figure><img src="./images/2/3.JPG" width=550px></figure>

<figure><img src="./images/2/4.JPG" width=550px></figure>

**例**：<figure><img src="./images/2/5.JPG" width=650px></figure>

#### Don't afraid of saddle point?

&emsp;&emsp;$$H$$或许可以告诉我们参数更新的方向！

<figure><img src="./images/2/6.JPG" width=450px></figure>

**例**：<figure><img src="./images/2/7.JPG" width=580px></figure>

### 2.1.1 Saddle point v.s. Local minima

<figure><img src="./images/2/8.JPG" width=600px></figure>

&emsp;&emsp;低维空间里的局部极小值点可能只是高维空间中的一个鞍点，那么当你有很多参数的时候，会不会局部极小值点就很少见了呢？

<figure><img src="./images/2/9.JPG" width=700px></figure>

### 2.1.2 Small gradient

<figure><img src="./images/2/10.JPG" width=400px></figure>

## 2.2 Tips for Training

### 2.2.1 Batch

#### Review: Optimization with batch

<figure><img src="./images/2/11.JPG" width=500px></figure>

#### Small batch v.s. Large batch

&emsp;&emsp;考虑有20个样本的情况（$$N=20$$）：

<figure><img src="./images/2/12.JPG" width=600px></figure>

左边的方法更新的一步更稳定，但是看起来花的时间长。但是实际上，考虑到并行运算，**大的batch size在计算梯度的时候并不需要更长的时间**。

<figure><img src="./images/2/13.JPG" width=500px></figure>


<!-- 蓝 -->
<b><font color="#3399ff"></font></b>
<!-- 绿 --><!-- #33cc00 -->
<b><font color="#00B050"></font></b>
<!-- 橙 -->
<b><font color="#FF4500"></font></b>