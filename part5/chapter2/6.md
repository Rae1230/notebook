---
sort: 6
---

# Plicy Optimization Ⅱ: SOTA

1. Policy Gradient→Natural Policy Gradient/TRPO→ACKTR→PPO  
   &emsp;[TRPO](http://proceedings.mlr.press/v37/schulman15.pdf): Trust region policy optimization. Schulman, L., Moritz, Jordan, Abbeel. 2015  
   &emsp;[ACKTR](https://proceedings.neurips.cc/paper/2017/file/361440528766bbaaaa1901845cf4152b-Paper.pdf): Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation. Y. Wu, E. Mansimov, S. Liao, R. Grosse, and J. Ba. 2017  
   &emsp;[PPO](https://arxiv.org/pdf/1707.06347.pdf): Proximal policy optimization algorithms. Schulman, Wolski,
Dhariwal, Radford, Klimov. 2017  
1. Q-learning→DDPG→TD3→SAC  
   &emsp;[DDPG](http://proceedings.mlr.press/v32/silver14.pdf): Deterministic Policy Gradient Algorithms, Silver et al. 2014  
   &emsp;[TD3](http://proceedings.mlr.press/v80/fujimoto18a/fujimoto18a.pdf): Addressing Function Approximation Error in Actor-Critic Methods, Fujimoto et al. 2018  
   &emsp;[SAC](http://proceedings.mlr.press/v80/haarnoja18b/haarnoja18b.pdf): Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor, Haarnoja et al. 2018  

```warning
这章的内容非常有挑战性，建议先阅读论文。
```

## 6.0 Review

### 6.0.1 Value-based RL VS Policy-based RL

* 在基于价值的RL<b><font color="#3399ff">确定性</font></b>策略是直接对价值函数作$$a_t = \operatorname{argmax}_a Q(s_t,a)$$贪心运算得到的。
* 在策略优化中，我们有来自$$\pi_{\theta}(a \vert s)$$<b><font color="#3399ff">随机</font></b>的策略输出，其中$$\theta$$是要优化的策略参数。
  
策略目标函数为$$J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}} [R(\tau)]$$，  
策略梯度（REINFORCE）为$$\nabla_{\theta} J(\theta) =\mathbb{E}_{\pi_{\theta}}\left[G_{t} \nabla_{\theta} \log \pi_{\theta}(s, a)\right] \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=0}^{T-1} G_{t}^{i} \nabla_{\theta} \log \pi_{\theta}\left(s_{i}, a_{i}\right)$$。

### 6.0.2 Advantage Actor-Critic

**引入baseline来减小AC的方差**

&emsp;&emsp;Advantage function将$$Q$$与baseline $$V$$结合：

$$A^{\pi}(s,a) = Q^{\pi}(s,a) - V^{\pi}(s)$$

那么策略梯度变为：

$$  \nabla_{\theta}J(\theta)
=   \mathbb{E}_{\pi_{\theta}} [\nabla_{\theta} \log \pi_{\theta}(s,a) A^{\pi_{\theta}}(s,a)]
$$

两个函数逼近器包含两组参数向量，利用TD learning或MC来更新这两组参数

$$\begin{aligned}
   V_{\mathbf{v}}(s) & \approx V^{\pi}(s) \\
   Q_{\mathbf{w}}(s,a) & \approx Q^{\pi}(s,a)
\end{aligned}$$

&emsp;&emsp;对于真价值函数$$V^{\pi_{\theta}}(s)$$，其**TD error** $$\delta^{\pi_{\theta}} = r(s,a) + \gamma V^{\pi_{\theta}}(s') - V^{\pi_{\theta}}(s)$$，**是优势函数的估计值**

$$\begin{aligned}
   \mathbb{E}_{\pi_{\theta}}\left[\delta^{\pi_{\theta}} \mid s, a\right] 
&= \mathbb{E}_{\pi_{\theta}}\left[r+\gamma V^{\pi_{\theta}}\left(s^{\prime}\right) \mid s, a\right]-V^{\pi_{\theta}}(s) \\
&= Q^{\pi_{\theta}}(s, a)-V^{\pi_{\theta}}(s) \\
&= A^{\pi_{\theta}}(s, a)
\end{aligned}$$

所以，我们可以用TD error来计算策略梯度

$$  \nabla_{\theta}J(\theta)
=   \mathbb{E}_{\pi_{\theta}} [\nabla_{\theta} \log \pi_{\theta}(s,a) \delta^{\pi_{\theta}}]
$$

这样，<b><font color="#FF4500">只要一组由TD估计的critic参数</font></b>$$\color{orange}{\kappa}$$就可以了

$$\delta_{\mathbf{v}} = r + \gamma V_{\kappa}(s') - V_{\kappa}(s)$$

### 6.0.3 Different time-scales

#### Crtic at different time-scales

&emsp;&emsp;Critic可以从以下多个目标来估计线性价值函数$$V_{\kappa}(s) = \psi(s)^T \kappa$$：  
对于MC，其更新为$$\Delta \kappa = \alpha(\color{green}{G_t} \color{black}{-} V_{\kappa}(s)) \psi(s)$$  
对于TD(0)，其更新为$$\Delta \kappa = \alpha(\color{green}{r + \gamma V_{\kappa}(s')} \color{black}{-} V_{\kappa}(s)) \psi(s)$$  
对于k-step return，其更新为$$\Delta \kappa = \alpha(\color{green}{\sum_{i=0}^k \gamma^i r_{t+i} + \gamma^k V_{\kappa}(s_{t+k})} \color{black}{-} V_{\kappa}(s)) \psi(s)$$  

#### Actors at different time-scales

&emsp;&emsp;策略梯度$$\nabla_{\theta}J(\theta) = \mathbb{E}_{\pi_{\theta}} [\nabla_{\theta} \log \pi_{\theta}(s,a) A^{\pi_{\theta}}(s,a)]$$也可以在许多时间尺度上进行估计：  
Monte-Carlo Actor-Critic策略梯度用整个return的误差来估计，$$\nabla_{\theta}J(\theta) = \alpha(G_t - V_{\kappa}(s_t)) \nabla_{\theta} \log \pi_{\theta}(s_t,a_t)$$。  
TD Difference Actor-Critic策略梯度用TD error来估计，$$\nabla_{\theta}J(\theta) = \alpha \big(r + \gamma V_{\kappa}(s_{t+1}) - V_{\kappa}(s_t) \big) \nabla_{\theta} \log \pi_{\theta}(s_t,a_t)$$。  
k-step return  Actor-Critic策略梯度用k-step return的误差来估计，$$\nabla_{\theta}J(\theta) = \alpha \big( \sum_{i=0}^k \gamma^i r_{t+i} + \gamma^k V_{\kappa}(s_{t+k}) - V_{\kappa}(s_t) \big) \nabla_{\theta} \log \pi_{\theta}(s_t,a_t)$$。  

### 6.0.5 Summary of PG Algorithms

* 策略梯度有很多不同形式
  $$\begin{aligned}
   \nabla_{\theta} J(\theta) &=\mathbb{E}_{\pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(s, a) G_{t}\right] \text{ ——REINFORCE} \\
   &=\mathbb{E}_{\pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(s, a) Q^{\mathrm{w}}(s, a)\right] \text{ ——Q Actor-Critic} \\
   &=\mathbb{E}_{\pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(s, a) A^{\mathrm{w}}(s, a)\right] \text{ ——Advantage Actor-Critic} \\
   &=\mathbb{E}_{\pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(s, a) \delta\right] \text{ ——TD Actor-Critic}
   \end{aligned}$$
* Critic用策略评估（e.g. MC或TD learning）来估计$$Q^{\pi}(s,a)$$，$$A^{\pi}(s,a)$$或$$V^{\pi}(s)$$。

<b><font color="#FF4500">Problems with PG</font></b>

&emsp;<b><font color="#FF4500">1. 采样效率低</font></b>，因为PG是在线学习，$$\nabla_{\theta} J(\theta) = \mathbb{E}_{a \sim \pi_{\theta}} [\nabla_{\theta} \log{ \pi_{\theta}(s,a) r(s,a) }]$$

&emsp;<b><font color="#FF4500">2. 策略更新过大或步长不当会破坏训练</font></b>  
&emsp;&emsp;这点和监督学习不一样，因为在监督学习中学习和数据是独立的。  
&emsp;&emsp;在RL中，步长过大→坏的策略→坏的数据收集  
&emsp;&emsp;可能无法从坏的策略中恢复，这会破坏整体的性能

<figure><img src="./images/6-1.JPG" width=300px></figure>

**如何使训练更稳定？**  
&emsp;&emsp;Trust region and natural policy gradient

**如何进行离策略的策略优化？**   
&emsp;&emsp;在TRPO中用的重要性采样

## 6.1 Natural Policy Gradient

&emsp;&emsp;策略梯度是欧几里得度量参数空间中最陡的上升：

$$ d^{*}
=  \nabla_{\theta} J(\theta)=\lim _{\epsilon \rightarrow 0} \frac{1}{\epsilon} \arg \max J(\theta+d), 
\text { s.t. }\|d\| \leq \epsilon
$$

它的缺点是对策略函数的参数化很敏感。

&emsp;&emsp;以KL-divergence（相对熵）为约束的distribution space（策略输出）中的最陡上升：

$$ d^{*}
=  \arg \max J(\theta+d), 
\text { s.t. } KL(\pi_{\theta} \| \pi_{\theta+d})=c
$$

将KL散度固定为常数$$c$$，可以确保我们以恒定速度沿着分布空间移动，而不管曲率如何（因此对模型参数化具有鲁棒性，因为我们只关心由参数引起的分布）。

#### KL-divergence as metric for two distributions

&emsp;&emsp;<b><font color="#00B050">KL散度</font></b>用于度量两个分布的“距离”：

$$\color{green}{ 
   KL(\pi_{\theta} \| \pi_{\theta’})
=  \mathbb{E}_{\pi_{\theta}}[\log \pi_{\theta}] - \mathbb{E}_{\pi_{\theta}}[\log \pi_{\theta'}]
}$$

尽管KL散度是非对称的，因此不是真正的度量，但这不影响我们使用它。这是因为当$$d$$趋向于0时，KL散度是渐进对称的。所以，在局部邻域内KL散度是近似对称的。

&emsp;&emsp;我们可以证明KL散度的二阶Taylor展开为

$$      KL(\pi_{\theta} \| \pi_{\theta+d}) 
\approx \frac{1}{2} d^T \mathbf{F} d$$

其中$$\mathbf{F}$$是KL散度的二阶导数$$\mathbb{E}_{\pi_{\theta}} [\nabla \log \pi_{\theta} \nabla \log \pi_{\theta}^T]$$，即**Fisher Information Matrix**（费舍尔信息矩阵）。

&emsp;&emsp;把以KL-divergence（相对熵）为约束的distribution space（策略输出）中的最陡上升$$ d^{*} = \arg \max J(\theta+d), \text { s.t. } KL(\pi_{\theta} \| \pi_{\theta+d})=c$$写成Lagrangian形式，$$JJ(\theta+d)$$由其一阶泰勒展开近似，约束KL散度由其二阶泰勒展开近似，得到

$$\begin{aligned}
    d^{*} 
&=  \underset{d}{\arg \max } J(\theta+d)-\lambda\left(K L\left(\pi_{\theta} \| \pi_{\theta+d}\right)-c\right) \\
& \approx \underset{d}{\arg \max } J(\theta)+\nabla_{\theta} J(\theta)^{T} d-\frac{1}{2} \lambda d^{T} F d+\lambda c
\end{aligned}$$

为了求最大化，我们令其关于$$d$$的导数为零，得到<b><font color="#00B050">natural Policy gradient</font></b> $$d = \frac{1}{\lambda} \mathbf{F}^{-1} \nabla_{\theta} J(\theta)$$。

&emsp;&emsp;自然策略梯度是**二阶**优化，更加准确而且与模型如何参数化无关（model invariant）。

$$\theta_{t+1} = \theta_t + \alpha \mathbf{F}^{-1} \nabla_{\theta} J(\theta)$$

其中，$$\mathbf{F} = \mathbb{E}_{\pi_{\theta}(s,a)} [\nabla \log \pi_{\theta}(s,a) \nabla \log \pi_{\theta}(s,a)^T]$$为Fisher信息矩阵，即KL散度的二阶微分。$$\mathbf{F}$$度量了策略（分布）关于模型参数$$\theta$$的曲率。

```note
无论模型如何参数化，自然策略梯度都会产生相同的策略变化。
```

自然梯度的详解可以参考[Natural Gradient Descent](https://agustinus.kristia.de/techblog/2018/03/14/natural-gradient/)



<!-- 蓝 -->
<b><font color="#3399ff"></font></b>
<!-- 绿 --><!-- #33cc00 -->
<b><font color="#00B050"></font></b>
<!-- 橙 -->
<b><font color="#FF4500"></font></b>