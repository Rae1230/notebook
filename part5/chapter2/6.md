---
sort: 6
---

# Plicy Optimization Ⅱ: SOTA

1. Policy Gradient→TRPO→ACKTR→PPO
   * [TRPO](http://proceedings.mlr.press/v37/schulman15.pdf): Trust region policy optimization. Schulman, L., Moritz, Jordan, Abbeel. 2015
   * [ACKTR](https://proceedings.neurips.cc/paper/2017/file/361440528766bbaaaa1901845cf4152b-Paper.pdf): Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation. Y. Wu, E. Mansimov, S. Liao, R. Grosse, and J. Ba. 2017
   * [PPO](https://arxiv.org/pdf/1707.06347.pdf): Proximal policy optimization algorithms. Schulman, Wolski,
Dhariwal, Radford, Klimov. 2017
2. Q-learning→DDPG→TD3→SAC
   * [DDPG](http://proceedings.mlr.press/v32/silver14.pdf): Deterministic Policy Gradient Algorithms, Silver et al. 2014
   * [TD3](http://proceedings.mlr.press/v80/fujimoto18a/fujimoto18a.pdf): Addressing Function Approximation Error in Actor-Critic Methods, Fujimoto et al. 2018
   * [SAC](http://proceedings.mlr.press/v80/haarnoja18b/haarnoja18b.pdf): Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor, Haarnoja et al. 2018

```warning
这章的内容非常有挑战性，建议先阅读论文。
```




<!-- 蓝 -->
<b><font color="#3399ff"></font></b>
<!-- 绿 --><!-- #33cc00 -->
<b><font color="#00B050"></font></b>
<!-- 橙 -->
<b><font color="#FF4500"></font></b>