---
sort: 6
---

# Plicy Optimization Ⅱ: SOTA

1. Policy Gradient→TRPO→ACKTR→PPO  
   &emsp;[TRPO](http://proceedings.mlr.press/v37/schulman15.pdf): Trust region policy optimization. Schulman, L., Moritz, Jordan, Abbeel. 2015  
   &emsp;[ACKTR](https://proceedings.neurips.cc/paper/2017/file/361440528766bbaaaa1901845cf4152b-Paper.pdf): Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation. Y. Wu, E. Mansimov, S. Liao, R. Grosse, and J. Ba. 2017  
   &emsp;[PPO](https://arxiv.org/pdf/1707.06347.pdf): Proximal policy optimization algorithms. Schulman, Wolski,
Dhariwal, Radford, Klimov. 2017  
1. Q-learning→DDPG→TD3→SAC  
   &emsp;[DDPG](http://proceedings.mlr.press/v32/silver14.pdf): Deterministic Policy Gradient Algorithms, Silver et al. 2014  
   &emsp;[TD3](http://proceedings.mlr.press/v80/fujimoto18a/fujimoto18a.pdf): Addressing Function Approximation Error in Actor-Critic Methods, Fujimoto et al. 2018  
   &emsp;[SAC](http://proceedings.mlr.press/v80/haarnoja18b/haarnoja18b.pdf): Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor, Haarnoja et al. 2018  

```warning
这章的内容非常有挑战性，建议先阅读论文。
```

## 6.0 Review

### 6.0.1 Value-based RL VS Policy-based RL

* 在基于价值的RL<b><font color="#3399ff">确定性</font></b>策略是直接对价值函数作$$a_t = \argmax_a Q(s_t,a)$$贪心运算得到的。
* 在策略优化中，我们有来自$$\pi_{\theta}(a \vert s)$$<b><font color="#3399ff">随机</font></b>的策略输出，其中$$\theta$$是要优化的策略参数。
  
策略目标函数为$$J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}} [R(\tau)]$$，策略梯度（REINFORCE）为$$\nabla_{\theta} J(\theta) =\mathbb{E}_{\pi_{\theta}}\left[G_{t} \nabla_{\theta} \log \pi_{\theta}(s, a)\right] \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=0}^{T-1} G_{t}^{i} \nabla_{\theta} \log \pi_{\theta}\left(s_{i}, a_{i}\right)$$。

### 6.0.2 Advantage Actor-Critic

**Reducing the Variance of Actor-Critic by a Baseline**

&emsp;&emsp;Advantage function：将$$Q$$与baseline $$V$$结合

$$A^{\pi}(s,a) = Q^{\pi}(s,a) - V^{\pi}(s)$$

那么策略梯度变为：

$$  \nabla_{\theta}J(\theta)
=   \mathbb{E}_{\pi_{\theta}} [\nabla_{\theta} \log \pi_{\theta}(s,a) A^{\pi_{\theta}}(s,a)]
$$

两个函数逼近器包含两组参数向量，利用TD learning或MC来更新这两组参数

$$\begin{aligned}
   V_{\mathbf{v}}(s) & \approx V^{\pi}(s) \\
   Q_{\mathbf{w}}(s,a) & \approx Q^{\pi}(s,a)
\end{aligned}$$

&emsp;&emsp;对于真价值函数$$V^{\pi_{\theta}}(s)$$，其**TD error** $$\delta^{\pi_{\theta}} = r(s,a) + \gamma V^{\pi_{\theta}}(s') - V^{\pi_{\theta}}(s)$$，**是优势函数的估计值**

$$\begin{aligned}
   \mathbb{E}_{\pi_{\theta}}\left[\delta^{\pi_{\theta}} \mid s, a\right] 
&= \mathbb{E}_{\pi_{\theta}}\left[r+\gamma V^{\pi_{\theta}}\left(s^{\prime}\right) \mid s, a\right]-V^{\pi_{\theta}}(s) \\
&= Q^{\pi_{\theta}}(s, a)-V^{\pi_{\theta}}(s) \\
&= A^{\pi_{\theta}}(s, a)
\end{aligned}$$

所以，我们可以用TD error来计算策略梯度

$$  \nabla_{\theta}J(\theta)
=   \mathbb{E}_{\pi_{\theta}} [\nabla_{\theta} \log \pi_{\theta}(s,a) \delta^{\pi_{\theta}}]
$$

这样，<b><font color="#3399ff">只要一组由TD估计的critic参数</font></b>$$\color{orange}{\kappa}$$就可以了

$$\delta_{\mathbf{v}} = r + \gamma V_{\kappa}(s') - V_{\kappa}(s)$$

### 6.0.3 Critic at different time-scales

&emsp;&emsp;Critic可以从以下多个目标来估计线性价值函数$$V_{\kappa}(s) = \psi(s)^T \kappa$$：  
对于MC，其更新为$$\Delta \kappa = \alpha(\color{green}{G_t} \color{black}{-} V_{\kappa}(s)) \psi(s)$$  
对于TD(0)，其更新为$$\Delta \kappa = \alpha(\color{green}{r + \gamma V_{\kappa}(s')} \color{black}{-} V_{\kappa}(s)) \psi(s)$$  
对于k-step return，其更新为$$\Delta \kappa = \alpha(\color{green}{\sum_{i=0}^k \gamma^i r_{t+i} + \gamma^k V_{\kappa}(s_{t+k})} \color{black}{-} V_{\kappa}(s)) \psi(s)$$  

### 6.0.4 Actors at different time-scales

&emsp;&emsp;策略梯度$$\nabla_{\theta}J(\theta) = \mathbb{E}_{\pi_{\theta}} [\nabla_{\theta} \log \pi_{\theta}(s,a) A^{\pi_{\theta}}(s,a)]$$也可以在许多时间尺度上进行估计：  
Monte-Carlo Actor-Critic策略梯度用整个return的误差来估计，$$\nabla_{\theta}J(\theta) = \alpha(G_t - V_{\kappa}(s_t)) \nabla_{\theta} \log \pi_{\theta}(s_t,a_t)$$。  
TD Difference Actor-Critic策略梯度用TD error来估计，$$\nabla_{\theta}J(\theta) = \alpha \big(r + \gamma V_{\kappa}(s_{t+1}) - V_{\kappa}(s_t) \big) \nabla_{\theta} \log \pi_{\theta}(s_t,a_t)$$。  
k-step return  Actor-Critic策略梯度用k-step return的误差来估计，$$\nabla_{\theta}J(\theta) = \alpha \big( \sum_{i=0}^k \gamma^i r_{t+i} + \gamma^k V_{\kappa}(s_{t+k}) - V_{\kappa}(s_t) \big) \nabla_{\theta} \log \pi_{\theta}(s_t,a_t)$$。  

### 6.0.5 Summary of PG Algorithms

* 策略梯度有很多不同形式
  $$\begin{aligned}
   \nabla_{\theta} J(\theta) &=\mathbb{E}_{\pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(s, a) G_{t}\right] \text{ ——REINFORCE} \\
   &=\mathbb{E}_{\pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(s, a) Q^{\mathrm{w}}(s, a)\right] \text{ ——Q Actor-Critic} \\
   &=\mathbb{E}_{\pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(s, a) A^{\mathrm{w}}(s, a)\right] \text{ ——Advantage Actor-Critic} \\
   &=\mathbb{E}_{\pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(s, a) \delta\right] \text{ ——TD Actor-Critic}
   \end{aligned}$$
* Critic用策略评估（e.g. MC或TD learning）来估计$$Q^{\pi}(s,a)$$，$$A^{\pi}(s,a)$$或$$V^{\pi}(s)$$。

## 6.1 The State of the Art RL Methods







<!-- 蓝 -->
<b><font color="#3399ff"></font></b>
<!-- 绿 --><!-- #33cc00 -->
<b><font color="#00B050"></font></b>
<!-- 橙 -->
<b><font color="#FF4500"></font></b>