---
sort: 4
---

# Self-attention

## 4.1 Sequence as Input

<figure><img src="./images/4/1.JPG" width=600px></figure>

&emsp;&emsp;目前为止，我们的网络的输入都是一个vector，输出可能是一个数值或者一个类别。那如果我们的输入是一排向量呢？输入的数量（即序列的长度）会改变呢？我们又该如何处理呢？

### 4.1.1 Vector set as input

&emsp;&emsp;什么情况下输入是一个长度可变的序列呢？

<figure><img src="./images/4/2.JPG" width=630px></figure>
&emsp;&emsp;输入是一个句子，把句子中的每个词汇描述成一个向量，那么model的输入就是大小会变的vector set。如果采用<b><font color="#00B050">one-hot encoding</font></b>对词汇进行编码，那么每个词汇就对应一个独热向量。另一种方法是<b><font color="#00B050">word embedding</font></b>，每个词对应一个包含语义信息的向量。  
To learn more: https://youtu.be/X7PH3NuYW0Q (in Mandarin)

<figure><img src="./images/4/3.JPG" width=580px></figure>
&emsp;&emsp;一段声音信号也是一排向量，一个window内的信号可以描述成一个向量（有很多种描述方法），称为<b><font color="#00B050">frame</font></b>，窗口每次向右移10ms。

Graph也是一组向量 (consider each **node** as **a vector**)。

<figure><img src="./images/4/4.JPG" width=400px></figure>
&emsp;&emsp;social network就是一个graph，其中每个node就是一个人，节点与节点间的edge就是两个人之间的关系，每个节点可以看作一个向量，其中包含了这个人的各种相关信息，所以整个graph可以看作是一堆向量组成的。  

<figure><img src="./images/4/5.JPG" width=400px></figure>
&emsp;&emsp;一个分子也是一个graph，每个原子可以用一个one-hot vector表示，一个分子就是一堆向量。

### 4.1.2 What is the output?

&emsp;**1. 每个向量都有一个对应的label** <b><font color="#00B050">Sequence Labeling</font></b>（本节主要关注的情况）

<figure><img src="./images/4/6.JPG" width=400px></figure>

Example applications
<figure><img src="./images/4/7.JPG" width=550px></figure>

&emsp;**2. 每个序列对应一个label**

<figure><img src="./images/4/8.JPG" width=320px></figure>

Example applications
<figure><img src="./images/4/9.JPG" width=550px></figure>

&emsp;**3. 模型自己决定label的数量**<b> <font color="#00B050">seq2seq
</font></b>

<figure><img src="./images/4/10.JPG" width=400px></figure>

## 4.2 Self-attention

**Sequence labeling**
<figure><img src="./images/4/11.JPG" width=500px></figure>
&emsp;&emsp;如果用全连接结构来解决这个问题，可以考虑上下文吗？FC可以考虑附近的内容。如何让考虑整个序列呢？用一个覆盖整个序列的窗口吗？

### 4.2.1 Self-attention

<figure><img src="./images/4/12.JPG" width=630px></figure>

&emsp;&emsp;Self-attention层会吃一整个序列的咨询，然后输出与输入数量相同的vector，自注意力层可以和全连接层交替使用。

<figure><img src="./images/4/13.JPG" width=600px></figure>

&emsp;&emsp;首先根据$$\mathbf{a^1}$$找出序列中与$$\mathbf{a^1}$$相关的其它向量，每个向量与$$\mathbf{a^1}$$的关联程度可以用数值$$\alpha$$来表示。那么自注意力机制中的module怎么决定两个向量之间的关联性呢？

### 4.2.2 Attention function

<figure><img src="./images/4/14.JPG" width=460px></figure>
&emsp;&emsp;计算attention有很多种方法，e.g. dot-product、additive… 比较常用的是dot-product，transformer中用的也是这个。

<figure><img src="./images/4/15.JPG" width=630px></figure>






<!-- 蓝 -->
<b><font color="#3399ff"></font></b>
<!-- 绿 --><!-- #33cc00 -->
<b><font color="#00B050"></font></b>
<!-- 橙 -->
<b><font color="#FF4500"></font></b>