---
sort: 4
---

# Value Function Approximation

## 4.1 Introduction: Scaling up RL

&emsp;&emsp;对于大规模的强化学习问题，应该如何扩展无模型方法来进行预测和控制呢？

&emsp;&emsp;在表格型方法中，我们用一个查找表来表示价值函数。每个状态$$s$$都有一个条目$$V(s)$$，每个状态-动作对$$s, a$$都有一个条目$$Q(s, a)$$。

<figure><img src="./images/4.1.JPG" width=px></figure>

&emsp;&emsp;而在大规模的MDPs中，有过多的状态或动作需要存储，独立地学习每个状态的值所需的时间也过多，所以表格型算法无法解决。

问：如何避免为每个状态显式学习或存储一下内容呢？  
1. 动态或奖励模型
2. 价值函数、状态-动作函数
3. 策略  

解：用函数近似来估计
$$\begin{aligned}
    \hat{v}(s,w) & \approx v_{\pi}(s)       \\
    \hat{q}(s,a,w) & \approx q_{\pi}(s,a)   \\
    \hat{\pi}(a,s,w) & \approx \pi(a \vert s)   \\
\end{aligned}
$$
&emsp;&emsp;从可见状态推广到不可见状态  
&emsp;&emsp;用MC或TD学习来更新参数$$w$$



Value function approximation for prediction
Value function approximation for control
Deep Q networks

<br />
<!-- 蓝 -->
<font color="#3399ff"></font>
<!-- 绿 --><!-- #33cc00 -->
<b><font color="#00B050"></font></b>
<!-- 橙 -->
<font color="#FF4500"></font>
<figure><img src="./images/.JPG" width=px></figure>