---
sort: 3
---

# Model-free prediction & control

* Model-free prediction: Estimate value function of an unknown MDP
* Model-free control: Optimize value function of an unknown MDP

请先阅读书本的Chapter 5和Chapter 6

<br />

&emsp;&emsp;Policy iteration和value iteration都是假设可以直接接触环境的动态过程和奖励的。然而在很多实际问题中，MDP模型可能是未知的或者过大而无法利用。

```note
强化学习中“episode（事件）”指的是agent在环境里面根据某个策略从开始到结束这一过程，“trajectory”是一段状态动作序列。
```

**Model-free RL: learning by iteration**
* 无模型的RL可以在与环境交互的过程中解决问题
* 不再能够直接访问已知的转移动态过程和奖励函数 
* trajectory/episodes是在agent与环境交互的过程中收集的
* 每个trajectory/episode包含$$\{S_1, A_1, R_1, S_2, A_2, R_2, \ldots, S_T, A_T, R_T\}$$

**Model-free prediction: policy evaluation without the access to the model**

在无法接触MDP模型时，估计某个策略下的期望return  
* Monte Carlo policy iteration
* Temporal Difference (TD) learning

## 3.1 Monte-Carlo Methods

* Return：$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots$$ 在策略$$\pi$$下
* $$v^{\pi}(s) = \mathbb{E}_{\tau \sim \pi} [G_t \vert S_t = s]$$，按策略$$\pi$$生成的trajectory$$\tau$$的期望
* MC simulation：对很多trajectory进行简单采样，计算所有轨迹的实际return，然后取平均
* MC policy iteration使用经验平均return替代期望return
* MC不需要MDP的动态过程/奖励，没有bootstrapping（在之前估计的基础上估计），也无需假设状态是马尔可夫的
* 只能应用于由一系列独立事件组成的MDPs（每个episode都会终止）

&emsp;&emsp;假设经验被划分为episodes，并且无论选择什么动作，所有episodes最终都会结束。只有在一个episodes完成时，价值估计和策略才会改变。因此，<font color="#3399ff">Monte-Carlo方法可以在“episode-by-episode”意义上是增量的</font>，而不是在step-by-step（在线）意义上。这里的“Monte-Carlo”特指基于averaging complete return的方法。

### 3.1.1 MC policy iteration

&emsp;&emsp;Monte-Carlo预测法可以在episode-by-episode的基础上进行增量实现。

评估状态$$v(s)$$：  
1. 在一个随机事件中访问状态$$s$$的每个时间步长$$t$$
2. 增量计数 $$N(s) \leftarrow N(s) + 1$$
3. 增量总return $$S(s) \leftarrow S(s) + G_t$$
4. 值的评估根据平均return $$v(s) = S(s) / N(s)$$

根据大数定律，当$$N(s) \rightarrow \infty$$时，$$v(s) \rightarrow v^{\pi}(s)$$。

#### Incremental mean
来自样本$$x_1, x_2, \ldots$$的平均值

$$\begin{aligned}
    \mu_t 
& = \frac{1}{t} \sum_{j = 1}^t x_j    \\
& = \frac{1}{t} \left(x_t + \sum_{j = 1}^{t-1} x_j\right)   \\
& = \frac{1}{t} (x_t + (t - 1) \mu_{t - 1}) \\
& = \mu_{t - 1} + \frac{1}{t} (x_t - \mu_{t - 1})
\end{aligned}$$

#### Incremental MC Updates
1. 收集一次episode $$(S_1, A_1, R_1, \ldots, S_t)$$
2. 对于每个有$$G_t$$（计算所得的return）的状态$$S_t$$  
   &emsp;&emsp;$$N(S_t) \leftarrow N(S_t) + 1$$  
   &emsp;&emsp;$$v(S_t) \leftarrow v(S_t) + \frac{1}{N(S_t)} (G_t - v(S_t))$$
3. 或者使用一个更新的均值（旧的事件会被忘记），适合解决非静态问题（$$\alpha$$可以理解为学习率）  
   &emsp;&emsp;$$v(S_t) \leftarrow v(S_t) + \alpha (G_t - v(S_t))$$

### 3.1.2 DP与MC在策略评估中的差异

&emsp;&emsp;动态规划（DP）通过价值估计$$v_{i-1}$$引导其余的期望回报的方法计算$$v_i$$。根据Bellman expectation backup迭代：

$$  v_i(s) 
=   \sum_{a \in A} \pi(a \vert s) 
        \bigg[R(s,a) + \gamma \sum_{s' \in S} P(s' \vert s,a) v_{i-1}(s')\bigg]
$$

<figure>
    <img src="./images/3-2.JPG" width=360px>
</figure>

&emsp;&emsp;MC用一次采样episode来更新经验平均return

$$v(S_t) \leftarrow v(S_t) + \alpha (G_{i,t} - v(S_t))$$

<figure>
    <img src="./images/3-3.JPG" width=360px>
</figure>

#### MC与DP的区别
1. MC根据样本经验进行操作，因此可以在没有模型的情况下直接学习。
2. MC不会在其他价值估计的基础上更新其价值估计。  
（这两个不同点并没有紧密联系，可以分开。）

#### MC相较于DP的优势
1. MC可以用于环境未知的情况。
2. 即使是环境动态特性完全已知的情况下，使用采样episode仍有很大的优势，e.g. 转移概率难以计算的情况。
3. 估计某个单一的状态的值与全部状态无关。所以可从感兴趣的状态开始采样，然后再取return的平均。

## 3.2 Temporal-Difference Learning






<br />
<!-- 蓝 -->
<font color="#3399ff"></font>
<!-- 绿 --><!-- #33cc00 -->
<b><font color="#00B050"></font></b>
<!-- 橙 -->
<font color="#FF4500"></font>
<figure>
    <img src="./images/.JPG" width=px>
</figure>