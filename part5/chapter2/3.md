---
sort: 3
---

# Model-free prediction & control

* Model-free prediction: Estimate value function of an unknown MDP
* Model-free control: Optimize value function of an unknown MDP

请先阅读书本的Chapter 5和Chapter 6

&emsp;&emsp;Policy iteration和value iteration都是假设可以直接接触环境的动态过程和奖励的。然而在很多实际问题中，MDP模型可能是未知的或者过大而无法利用。

**Model-free RL: learning by iteration**
* 无模型的RL可以在与环境交互的过程中解决问题
* 不再能够直接访问已知的转移动态过程和奖励函数 
* trajectory/episodes是在agent与环境交互的过程中收集的
* 每个轨迹/随机事件包含$$\{S_1, A_1, R_1, S_2, A_2, R_2, \ldots, S_T, A_T, R_T\}$$

**Model-free prediction: policy evaluation without the access to the model**

在无法接触MDP模型时，估计某个策略下的期望return  
* Monte Carlo policy iteration
* Temporal Difference (TD) learning

---

## 3.1 Monte-Carlo Methods  
* Return：$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots$$ 在策略$$\pi$$下
* $$v^{\pi}(s) = \mathbb{E}_{\tau \sim \pi} [G_t \vert S_t = s]$$，按策略$$\pi$$生成的轨迹$$\tau$$的期望
* MC simulation：对很多轨迹进行简单采样，计算所有轨迹的实际return，然后取平均
* MC policy iteration使用经验平均return替代期望return
* MC不需要MDP的动态过程/奖励，没有引导，也无需假设状态是马尔可夫的
* 只能应用于随机MDPs（每个随机事件都会终止）

### 3.1.1 MC policy iteration

评估状态$$v(s)$$：  
1. 在一个随机事件中访问状态$$s$$的每个时间步长$$t$$
2. 增量计数 $$N(s) \leftarrow N(s) + 1$$
3. 增量总return $$S(s) \leftarrow S(s) + G_t$$
4. 值的评估根据平均return $$v(s) = S(s) / N(s)$$

根据大数定律，当$$N(s) \rightarrow \infty$$时，$$v(s) \rightarrow v^{\pi}(s)$$。

#### Incremental mean

来自样本$$x_1, x_2, \ldots$$的平均值

$$\begin{aligned}
    \mu_t 
& = \frac{1}{t} \sum_{j = 1}^t x_j    \\
& = \frac{1}{t} \left(x_t + \sum_{j = 1}^{t-1} x_j\right)   \\
& = \frac{1}{t} (x_t + (t - 1) \mu_{t - 1}) \\
& = \mu_{t - 1} + \frac{1}{t} (x_t - \mu_{t - 1})
\end{aligned}$$

#### Incremental MC Updates

1. 收集一次随机事件 $$(S_1, A_1, R_1, \ldots, S_t)$$
2. 对于每个有$$G_t$$（计算所得的return）的状态$$S_t$$
   $$N(S_t) \leftarrow N(S_t) + 1$$
   $$v(S_t) \leftarrow v(S_t) + \frac{1}{N(S_t)} (G_t - v(S_t))$$
3. 或者使用一个更新的均值（旧的事件会被忘记）。适用于非静态问题
   $$v(S_t) \leftarrow v(S_t) + \alpha (G_t - v(S_t))$$

### 3.2 DP与MC在策略评估中的差异

&emsp;&emsp;动态规划（DP）通过价值估计$$v_{i-1}$$引导其余的期望回报的方法计算$$v_i$$。根据Bellman expectation backup迭代：

$$  v_i(s) 
=   \sum_{a \in A} \pi(a \vert s) 
        \bigg[R(s,a) + \gamma \sum_{s' \in S} P(s' \vert s,a) v_{i-1}(s')\bigg]
$$

<figure>
    <img src="./images/3-2.JPG" width=360px>
</figure>

&emsp;&emsp;MC用一次采样事件来更新经验平均return

$$v(S_t) \leftarrow v(S_t) + \alpha (G_{i,t} - v(S_t))$$

<figure>
    <img src="./images/3-3.JPG" width=360px>
</figure>

#### MC相较于DP的优势
1. MC可以用于环境未知的情况
2. 即使是环境的动态特性完全已知的情况下，使用采样事件仍有很大的优势，e.g. 转移概率难以计算的情况。
3. 估计某个单一的状态的值是与所有的状态无关的。所以可以从感兴趣的状态开始采样，然后再取return的平均。

## 3.2 Temporal-Difference Learning






<br />
<!-- 蓝 -->
<font color="#3399ff"></font>
<!-- 绿 --><!-- #33cc00 -->
<b><font color="#00B050"></font></b>
<!-- 橙 -->
<font color="#FF4500"></font>
<figure>
    <img src="./images/.JPG" width=px>
</figure>