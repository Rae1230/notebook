---
sort: 4
---

# Actor-Critic

Review-Policy Gradient

<figure>
    <img src="./images/4/4-1.JPG" width=400px>
</figure>

Review-Q-Learning

<figure>
    <img src="./images/4/4-2.JPG" width=420px>
</figure>

**Actor-Critic**

<figure>
    <img src="./images/4/4-3.JPG" width=400px>
</figure>

## 4.1 Advantage Actor-Critic

<figure>
    <img src="./images/4/4-4.JPG" width=600px>
</figure>

<figure>
    <img src="./images/4/4-5.JPG" width=320px>
</figure>

Tips:
* actor $$\pi(s)$$和critic $$V^{\pi}(s)$$的参数可以共享
* 用输出熵作为$$\pi(s)$$的正则化  
  倾向于更大的熵 $$\rightarrow$$ exploration

## 4.2 Asynchronous Advantage Actor-Critic (A3C)

&emsp;&emsp;在A2C的基础上，利用多个worker来收集经验。

<figure>
    <img src="./images/4/4-6.JPG" width=450px>
</figure>

## 4.3 Pathwise Derivative Policy Gradient

David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, Martin Riedmiller, “Deterministic Policy Gradient Algorithms”, ICML, 2014.   
Timothy P . Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess,
Tom Erez, Yuval Tassa, David Silver, Daan Wierstra, “CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING”, ICLR, 2016.

### 4.3.1 Another way to use critic

<figure>
    <img src="./images/4/4-7.JPG" width=400px>
</figure>

<figure>
    <img src="./images/4/4-8.JPG" width=400px>
</figure>

### 4.3.2 PDPG

<figure>
    <img src="./images/4/4-9.JPG" width=400px>
</figure>

<figure>
    <img src="./images/4/4-10.JPG" width=360px>
</figure>

<figure>
    <img src="./images/4/4-11.JPG" width=420px>
</figure>

### 4.3.3 Connection with GAN

<figure>
    <img src="./images/4/4-12.JPG" width=280px>
    <figcaption>David Pfau, Oriol Vinyals, “Connecting Generative Adversarial  Networks and Actor-Critic Methods”, arXiv preprint, 2016</figcaption>
</figure>


<!-- 蓝 -->
<b><font color="#3399ff"></font></b>
<!-- 绿 --><!-- #33cc00 -->
<b><font color="#00B050"></font></b>
<!-- 橙 -->
<b><font color="#FF4500"></font></b>