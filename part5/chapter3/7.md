---
sort: 7
---

# Model-based Reinforcement Learning

## 7.1 Introduction to Model-based RL

先前关于model-free RL的课程
* 利用策略梯度直接从经验中学习策略
* 通过MC或TD学习价值函数

这节将关注model-based RL
* 从经验中学习环境的模型
* 利用学到的模型来改进价值/策略优化

### 7.1.1 Building a model of the environment

<!-- <figure><img src="./images/7-1" width=900px></figure> -->

agent不仅可以和环境交互，也可以和模型交互

### 7.1.2 Modeling the environment for planning

&emsp;&emsp;如果我们有环境模型，那么就可以利用环境模型进行规划，帮助更好地估计策略或价值函数。

<!-- <figure><img src="./images/7-2" width=900px></figure> -->

规划是将模型作为输入并通过与环境模型交互来产生或改进策略的计算过程

<!-- <figure><img src="./images/7-3" width=900px></figure> -->

状态空间规划：在状态空间内搜索到达目标的最优策略或最优路径

Model-based Value Optimization方法享有一个一般结构

<!-- <figure><img src="./images/7-4" width=900px></figure> -->

Model-based Policy Optimization的结构更加简单

<!-- <figure><img src="./images/7-5" width=900px></figure> -->

### 7.1.3 Structure of Model-based RL

<!-- <figure>
    <img src="./images/7-5" width=900px>
    <figcaption>学习、规划和动作之间的关系</figcaption>
</figure> -->

真实经验的两个作用：
1. 直接利用之前的方法来改进价值和策略
2. 改进模型，令模型更加准确的表示真实环境（环境预测模型：$$p(s_{t+1} \vert s_t,a_t), R(s_t,a_t)$$）

### 7.1.4 Advantage of Model-based RL

<!-- <figure><img src="./images/7-6" width=900px></figure> -->

<b><font color="#3399ff">更好的采样效率</font></b>

* 采样高效的学习对于现实生活中的RL应用很重要，比如机器人
* 可以通过监督学习方法来高效地学习模型

<!-- <figure><img src="./images/7-8" width=900px></figure> -->

<b><font color="#FF4500">Model-based RL存在的问题：</font></b>

1. 先学习模型，后构建价值函数或策略函数，这会导致两个近似误差来源
2. 很难保证收敛

### 7.1.5 What is a model

&emsp;&emsp;模型$$\mathcal{M}$$是由$$\eta$$参数化的MDP过程的表示。一般来说，模型$$\color{green}{\mathcal{M} = (P, R)}$$<b><font color="#00B050">包含状态转移和奖励</font></b>

$$\begin{aligned}
    S_{t+1} & \sim P_{\eta}(S_{t+1} \vert S_t,A_t)  \\
    R_{t+1} & =    R_{\eta}(S_{t+1} \vert S_t,A_t)
\end{aligned}
$$

通常情况下，我们**假设状态转移与奖励之间条件无关**

$$  P(S_{t+1},R_{t+1} \vert S_t,A_t)
  = P(S_{t+1} \vert S_t,A_t) P(R_{t+1} \vert S_t,A_t)
$$

#### Sometimes easy to access the model

比如，已知模型：围棋，游戏的规则就是模型；物理模型，车辆的动力学模型和自行车的运动学模型

<!-- <figure><img src="./images/7-9" width=900px></figure> -->

## 7.2 Model-based Value Optimization

### 7.2.1 Learning the model

&emsp;&emsp;目标：从经验$$\{S_1, A_1, R_2, \ldots, S_T\}$$中学习模型$$\mathcal{M}\eta$$。这可以看作一个监督学习问题。学习$$s,a \rightarrow r$$是回归问题，学习$$s,a \rightarrow s^{\prime}$是密度估计问题。损失函数可以是均方误差、KL散度，然后优化$$\eta$$来使得经验损失最小。

Examples of models: 
1. Table Lookup Model
2. Linear Expectation Model
3. Linear Gaussian Model
4. Gaussian Process Model
5. Deep Belief Network Model ...





## 7.3 Model-based Policy Optimization
## 7.4 Case Study on Robot Object Manipulation
<!-- 蓝 -->
<b><font color="#3399ff"></font></b>
<!-- 绿 --><!-- #33cc00 -->
<b><font color="#00B050"></font></b>
<!-- 橙 -->
<b><font color="#FF4500"></font></b>