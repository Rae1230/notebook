---
sort: 2
---

# ACKTR

[Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation.](https://proceedings.neurips.cc/paper/2017/file/361440528766bbaaaa1901845cf4152b-Paper.pdf) Y. Wu, E. Mansimov, S. Liao, R. Grosse, and J. Ba. 2017

[Github](https://github.com/openai/baselines) OpenAI的代码

&emsp;&emsp;ACKTR扩展了自然策略梯度的框架并用Kronecker因子近似曲率（Kronecker-factored approximate curvature, K-FAC），同时在置信域内优化actor和critic，因此该方法称为Actor Critic using Kronecker-Factored Trust Region (ACKTR)。这是第一个针对actor-critic法的可扩展置信域自然梯度方法，也是从原始图像输入直接学习复杂任务的连续控制和离散控制策略的方法。与先前的actor-critic法相比，ACKTR取得了更高的奖励和平均2~3倍的样本采样效率。

## 1 Introduction

&emsp;&emsp;深度强化学习（Deep RL, DRL）用深度神经网络表示控制策略。尽管效果很好，但这些网络仍用随机梯度下降（stochastic gradient descent, SGD）训练。SGD及相关的一阶方法效率不高，所以有了用多个agent同时与环境交互的分布式方法，但随着并行度的增加，采样效率的回报会迅速减少。

&emsp;&emsp;采样效率是RL中的主要关注点。有效减少样本量的一种方法是使用更高级的梯度更新优化技术。自然策略梯度利用自然梯度下降进行策略更新。自然梯度法沿着最陡的梯度下降，用Fisher信息矩阵作基础矩阵，该矩阵与坐标轴的选择无关，只与流形相关（i.e. 表面）。  
&emsp;&emsp;但是，自然梯度的计算涉及Fisher信息矩阵的求逆，所以计算麻烦。TRPO利用Fisheries向量积避免了显式存储和对Fisher矩阵求逆。但是，TRPO通常需要多次共轭梯度才能获取单个参数的更新，并且准确估计曲率需要每个batch中的大量样本，所以TRPO对于大的模型不适用且采样效率低。

&emsp;&emsp;K-FAC是自然梯度的可扩展近似。在监督学习中，通过使用更大的mini-batches，K-FAC可以加速很多SOTA的大规模神经网络。与TRPO不同，每次更新的成本和一次SGD更新相当，并且K-FAC保持曲率信息的运行平均值，这使得K-FAC中可以用小的batch。这表明将K-FAC用于策略优化可以提高现在DRL的采样效率。

&emsp;&emsp;ACKTR中对自然梯度用Kronecker-factored近似，可以高效地对梯度的协方差矩阵求逆。该文章还提出扩展自然策略梯度算法以通过Gauss-Newton近似来优化价值函数。在实践中，ACKTR每次更新的计算成本仅比基于SGD的方法高10%~25%。








<!-- 蓝 -->
<b><font color="#3399ff"></font></b>
<!-- 绿 --><!-- #33cc00 -->
<b><font color="#00B050"></font></b>
<!-- 橙 -->
<b><font color="#FF4500"></font></b>