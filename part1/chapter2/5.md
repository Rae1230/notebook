---
sort: 5
---

# Continuous Control with DRL

Continuous Control with Deep Reinforcement Learning. [C]//ICLR. Lillicrap T P, Hunt J J, Pritzel A, et al. 2015

<br/>

*摘要*——DDPG试图将deep Q-learning应用到连续动作空间。本文提出了一种基于确定性策略梯度的actor-critic, model-free的算法，该算法可用于连续动作空间。采用同样的学习方法、网络结构和超参数的情况下，DDPG可以鲁棒地解决超过20个模拟的物理问题，包括经典问题，如车杆起摆、灵巧操作、腿的控制和车辆驾驶。DDPG可以找到与能完全获知系统动态的规划方法性能相当的算法。进一步地，对应很多任务，DDPG都能学习“端到端”的策略：直接从输入图像学习。

## 1. Introduction

&emsp;&emsp;AI领域的主要目标之一就是通过未处理的、高维的传感器输入来完成任务。最近，最近已经取得了显著进展，DQN可以通过未处理过的输入像素来玩Atari游戏且水平与人类差不多。为了达到这个目标，深度神经网络函数逼近器被用来估计动作-价值函数。  
&emsp;&emsp;但是，尽管DQN可以解决具有高维输入空间的问题，它只能处理离散且低维的动作空间问题。DQN无法直接应用道连续领域，因为它通过最大化动作-价值函数来选择动作，而这对于连续值的情况意味着每步都需要迭代优化。  
&emsp;&emsp;一个比较显然的解决方式就是对动作空间做离散化处理。但是，这种做法局限性很大，比较值得注意的一点就是维度灾难。过大的动作空间是很难被有效探索的，因此在这种情况下训练类似DQN这样的网络是很困难的。此外，直接把动作空间离散化会丢失动作空间的结构信息，而这些结构信息可能对解决很多问题来说很重要。  

&emsp;&emsp;本文提出了一种model-free, off-policy actor-critic算法，利用深度函数逼近器来学习高维连续动作空间中的策略。本文工作基于确定性策略梯度（DPG）实现。  
&emsp;&emsp;这里，本文将actor-critic方法与DQN结合起来。在DQN之前，一般认为用大的非线性函数逼近器来学习价值函数是困难且不稳定的。DQN能稳定且鲁棒地学习是因为以下两点创新：1. 网络的训练是off-policy的，用的是经验池中的样本，最小化了样本间的关联性；2. 利用一个目标Q网络来训练网络，这样在TD回溯的时候目标是保持一致的。本文利用了类似的思想，以及批量归一化的方法，一个深度学习中的最新进展。  
&emsp;&emsp;为了评估DDPG，本文构造了很多不同的有挑战性的物理控制问题。有一些经典问题，也有新的挑战，比如从视频输入直接学习动作策略。  
&emsp;&emsp;本文提出的model-free的方法称为Deep DPG (DDPG) 可以通过低维的观测（e.g. 笛卡尔坐标系活关节角度）学到有竞争力的策略。在很多情况下，DDPG还能直接从像素学到好的策略，并保持常数超参数和网络结构。  
&emsp;&emsp;该方法的关键特征是简单：只需一个前向的actor-critic架构和具有很少“moving parts”的学习算法，这使得DDPG易于实现且可以被扩展到更难的问题和更大的网络。在物理控制问题上，DDPG有时会超越规划算法的性能。





<!-- 蓝 -->
<b><font color="#3399ff"></font></b>
<!-- 绿 --><!-- #33cc00 -->
<b><font color="#00B050"></font></b>
<!-- 橙 -->
<b><font color="#FF4500"></font></b>