---
sort: 4
---

# Deterministic Policy Gradient

Deterministic Policy Gradient Algorithms. [C]//International conference on machine learning (ICML). Silver D, Lever G, Heess N, et al. , 2014 
[附录](http://proceedings.mlr.press/v32/silver14-supp.pdf)

<br/>

*摘要*——本文针对强化学习中的连续问题，提出了**确定性**策略梯度算法。确定性策略梯度具有非常吸引人的形式：它是动作-价值函数的期望梯度。这种简单的形式意味着确定性策略梯度可以比一般的随机策略梯度更有效地估计。为保证充分的探索，本文提出了一种离线的actor-critic算法，该算法从一个探索性的行为策略中学习确定性目标策略。在高维动作空间中，该算法的性能显著优于随机策略。

## 1 Introduction

&emsp;&emsp;策略梯度算法的基本思想是用一个参数化的策略分布$$\pi_\theta(a \vert s) = \mathbb{P}[a \vert s; \theta]$$来表示策略，处于状态$$s$$下时根据参数向量$$\theta$$随机选择动作$$a$$。策略梯度算法通常通过对这种随机梯度进行采样，并往累计奖励更大的方向调整策略参数。

&emsp;&emsp;与一般的策略梯度算法不同，本文采用确定性策略$$a = \mu_\theta(s)$$。与过去的结论不同，本文证明了确定性策略梯度确实存在，并且有简单的model-free的形式，即跟随action-value函数的梯度。此外，本文还证明了确定性策略梯度是随机策略梯度的一种特殊情况，即策略方差趋于0的情况。

&emsp;&emsp;从实践的角度讲，随机与确定策略梯度有非常关键的不同之处。在确定性的情况下，策略梯度只在状态空间上积分。

&emsp;&emsp;为了保证确定性策略算法能够持续探索，本文提出了一个off-policy的学习算法。其基本思想是根据一个随机行为策略来选择动作（保证充分的探索），但是学习一个确定的目标策略（利用确定性策略梯度的效率）。本文利用确定性策略梯度推导出一个off-policy actor-critic算法，该算法用一个可微的函数逼近器来估计动作-价值函数，然后沿近似的动作-价值函数的梯度方向来更新策略参数。此外，本文还引入了确定性策略梯度的相容函数逼近概念，以此保证对策略梯度的无偏估计。

&emsp;&emsp;实验结果表明，确定性策略梯度的性能明显优于随机策略梯度，尤其是在高维任务中。进一步地，本文提出的方法计算量并不比先前的方法大：每次更新的计算成本在动作维度和策略参数量上是线性的。最后，还有很多应用（如在机器人中）提供了可微的控制策略，但没有给控制器增加噪声的功能。在这些情况下，随机策略梯度是不适用的，而本文的方法可能仍然有用。


## 2 Background

### 2.1 Preliminaries

&emsp;&emsp;随机策略$$\pi_\theta: \mathcal{S} \rightarrow P(\mathcal{A})$$。从状态$$s$$经过$$t$$ steps到达状态$$s^{\prime}$$的概率为$$p(s \rightarrow s^{\prime}, t, \pi)$$，则状态分布为$$\rho^\pi(s^{\prime}) := \int_{\mathcal{S}} \sum_{t=1}^{\infty} \gamma^{t-1} p_1(s) p(s \rightarrow s^{\prime}, t, \pi) \mathrm{~d}s$$。然后，可以将性能目标写成期望的形式

$$  J(\pi_\theta) 
=   \int_{\mathcal{S}} \rho^\pi(s) \int_{\mathcal{A}} \pi_\theta(s, a) r(s, a) \mathrm{d}a \mathrm{~d}s 
=   \mathbb{E}_{s \sim \rho^\pi, a \sim \pi_\theta}[r(s, a)]
\tag{1} $$

### 2.2 Stochastic PG Theorem

&emsp;&emsp;策略梯度方法的基本思想是沿着性能梯度$$\nabla_\theta J(\pi_\theta)$$上升的方向调整参数$$\theta$$。需要注意的是，尽管状态分布$$\rho^\pi(s)$$由策略参数决定，但是策略梯度与状态分布的梯度无关。

$$  \nabla_\theta J(\pi_\theta)  
=   \int_{\mathcal{S}} \rho^\pi(s) \int_{\mathcal{A}} \nabla_\theta \pi_\theta(a \vert s) Q^\pi(s, a) \mathrm{d}a \mathrm{~d}s 
=   \mathbb{E}_{s \sim \rho^\pi, a \sim \pi_\theta} [\nabla_\theta \log \pi_\theta(a \vert s) Q^\pi(s, a)]
\tag{2} $$

&emsp;&emsp;这个定理有很重要的实用价值，因为它把性能梯度的计算简化成了一个简单的期望。这类算法必须解决的一个问题就是如何估计动作-价值函数$$Q^\pi(s, a)$$。

### 2.3 Stochastic Actor-Critic Algorithms

&emsp;&emsp;一般来说，用函数逼近器$$Q^w(s, a)$$的值来代替真实的动作-价值函数$$Q^\pi(s, a)$$会带来偏差。但是，如果函数逼近器compatible（兼容）：i）$$Q^w(s, a)=\nabla_\theta \log \pi_\theta(a \mid s)^{\top} w$$ ii）参数$$w$$被选来最小化均方误差$$\epsilon^2(w)=\mathbb{E}_{s \sim \rho^\pi, a \sim \pi_\theta} \left[ \big( Q^w(s, a) - Q^\pi(s, a) \big)^2 \right]$$，那就是无偏的。

$$  \nabla_\theta J(\pi_\theta)  
=   \mathbb{E}_{s \sim \rho^\pi, a \sim \pi_\theta} [\nabla_\theta \log \pi_\theta(a \vert s) Q^w(s, a)]
\tag{3} $$

更直观地讲，条件 i）要求兼容函数逼近器在随机策略的“特征”上是线性的，$$\nabla_\theta \log \pi_\theta(a \vert s)$$，条件 ii）要求参数是由这些特征估计的$$Q^\pi(s, a)$$的线性回归问题的解。在实际应用中，条件 ii）通常会被放松，因为用TD学习的策略评估算法估计价值函数更高效。实际上，如果两个条件都满足的话整个算法可以等价于完全没有用critic的算法，更像是REINFORCEMENT算法。

### 2.4 Off-Policy Actor-Critic

&emsp;&emsp;一般来说，用按另一个行为策略$$\beta(a \vert s) \neq \pi_\theta(a \vert s)$$采样的轨迹来对策略梯度进行off-policy估计是有用的。在off-policy的设置下，性能目标通常被修改为目标策略的价值函数，对行为策略的状态分布取平均。

$$  J_\beta(\pi_\theta) 
=   \int_{\mathcal{S}} \rho^\beta(s) V^\pi(s) \mathrm{d} s 
=   \int_{\mathcal{S}} \int_{\mathcal{A}} \rho^\beta(s) \pi_\theta(a \vert s) Q^\pi(s, a) \mathrm{d}a \mathrm{~d}s
\tag{4} $$

&emsp;&emsp;对性能目标求微分并取近似，就得到off-policy的策略梯度

$$\begin{aligned}
    \nabla_\theta J_\beta\left(\pi_\theta\right) 
&   \approx \int_{\mathcal{S}} \int_{\mathcal{A}} 
    \rho^\beta(s) \nabla_\theta \pi_\theta(a \vert s) Q^\pi(s, a) \mathrm{d}a \mathrm{~d}s \\
& = \mathbb{E}_{s \sim \rho^\beta, a \sim \beta}
    \left[\frac{\pi_\theta(a \vert s)}{\beta_\theta(a \vert s)} \nabla_\theta \log \pi_\theta(a \vert s) Q^\pi(s, a)\right]
\end{aligned}   \tag{5} $$

这个近似省略了与动作-价值的梯度$$\nabla_\theta Q^\pi(s,a)$$有关的一项。Off-policy Actor-Critic (OffPAC) 算法用行为策略$$\beta(a \vert s)$$来生成轨迹。一个critic通过梯度TD学习来估计状态-价值函数，$$V^v(s) \approx V^\pi(s)$$，相对这些轨迹是off-policy的。一个actor根据式 (5)做随机梯度上升更新策略参数$$\theta$$，相对于轨迹也是off-policy的。TD error，$$\delta_t = r_{t+1} + \gamma V^v(s_{t+1})-V^v(s_t)$$，可以用来替代式 (5) 中未知的动作-价值函数$$Q^\pi(s, a)$$；这已经证实了能够提供对真实梯度的近似。Actor和critic都用了一个重要性采样比$$\frac{\pi_\theta(a \vert s)}{\beta_\theta(a \vert s)}$$，因为动作是根据$$\pi$$而不是$$\beta$$选择的。


## 3 Gradients of Deterministic Policies

&emsp;&emsp;本文的主要成果是一个确定性策略梯度定理，类似于前面提到的随机策略梯度定理。文章将会提供几种不同的角度来推导和理解这一结果。首先，本文给出了一个不正式的直觉上的解释。然后，从第一性原则出发给出确定性策略梯度的正式证明。最后，本文证明了<b><font color="#3399ff">确定性策略梯度定理实际上是随机策略梯度的一个极限情况</font></b>，证明细节见附录。

### 3.1 Action-Value Gradients

&emsp;&emsp;大多数无模型的强化学习算法都是基于一般策略迭代：策略评估和策略改进交替进行。策略评估方法估计动作-价值函数$$Q^\pi(s, a)$$或$$Q^\mu(s, a)$$，通过Monte-Carlo评估或temporal-difference learning。策略改进方法根据（估计的）动作-价值函数更新策略。最常用的方法就是取动作-价值函数的贪心最大（或soft最大），$$\mu^{k+1}(s) = \operatorname{arg} \max_a Q^{\mu^k}(s, a)$$。

&emsp;&emsp;在连续动作空间中，贪心策略改进会带来一个问题，每步都需要做一次全局最大化。解决办法是让策略沿$$Q$$的梯度方向移动，即对每个被访问的状态$$s$$，策略参数$$\theta^{k+1}$$都与梯度$$\nabla_\theta Q^{\mu^k}(s, \mu_\theta(s))$$成比例地更新。每个状态都会得到一个不同的策略改进的方向，这些方向可以通过对状态分布$$\rho^\mu(s)$$取期望得到平均值。

$$  \theta^{k+1}
=   \theta^k + \alpha \mathbb{E}_{s \sim \rho^{\mu^k}} \left[ \nabla_\theta Q^{\mu^k}(s, \mu_\theta(s)) \right]
\tag{6} $$

&emsp;&emsp;通过应用链式法则，可以把策略改进分解为动作-价值关于动作的梯度和策略关于策略参数的梯度。

$$  \theta^{k+1}
=   \theta^k + \alpha \mathbb{E}_{s \sim \rho^{\mu^k}} 
                            \left[ \nabla_\theta \mu_\theta(s) \left. \nabla_a Q^{\mu^k}(s, a) \right|_{a = \mu_\theta(s)} \right]
\tag{7} $$

一般来说，$$\nabla_\theta \mu_\theta(s)$$是一个Jacobian矩阵。但是，改变策略的话，会访问不同的状态并且状态分布$$\rho^\mu$$也会改变。因此，在不考虑分布改变的情况下，这种方法能否保证改进并不是立即显而易见的。然而，下节的理论表明，**与随机策略梯度一样，并不需要计算状态分布的梯度；并且上面所述的直观的更新恰好遵循性能目标的梯度。**

### 3.2 DPG Theorem

&emsp;&emsp;考虑确定性策略$$\mu_\theta : \mathcal{S} \rightarrow \mathcal{A}$$，有参数向量$$\theta \in \mathbb{R}^n$$。定义一个性能目标$$J(\mu_\theta) = \mathbb{E}[r_1^\gamma \vert \mu]$$，并定义概率分布为$$p(s \rightarrow s^\prime, t, \mu)$$和折扣状态分布$$\rho^\mu(s)$$。这样，就可以把性能目标写成期望的形式

$$  J(\mu_\theta)
=   \int_{\mathcal{S}} \rho^\mu(s) r\left(s, \mu_\theta(s)\right) \mathrm{d} s
=   \mathbb{E}_{s \sim \rho^\mu}\left[r\left(s, \mu_\theta(s)\right)\right]
\tag{8} $$

下面将提供策略梯度定理的确定性的类比情况。证明的形式类似于[ (Sutton et al., 1999) ](https://proceedings.neurips.cc/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf)，详见附录B。

<b><font color="#00B050">Theorem 1 (Deterministic Policy Gradient Theorem). </font></b>假设MDP满足条件A.1（见附录，这些条件表明$$\nabla_\theta \mu_\theta(s)$$和$$\nabla_a Q^\mu(s, a)$$存在并且确定性策略梯度存在）。那么，

$$\color{green}{\begin{aligned}
    \nabla_\theta J(\mu_\theta) 
& = \int_{\mathcal{S}} \rho^\mu(s) \nabla_\theta \mu_\theta(s) \left. \nabla_a Q^\mu(s, a)\right|_{a=\mu_\theta(s)} \mathrm{d}s \\
& = \mathbb{E}_{s \sim \rho^\mu} \left[\left.\nabla_\theta \mu_\theta(s) \nabla_a Q^\mu(s, a)\right|_{a=\mu_\theta(s)}\right]
\end{aligned}}  \tag{9} $$

### 3.3 Limit of the Stochastic PG

&emsp;&emsp;确定性策略梯度定理第一眼看上去并不像随机的版本那样（式 (2) ）。下面，将证明对于一大类随机策略来说，确定性策略梯度确实是随机策略梯度的一个特例（极限情况）。用一个确定性策略$$\mu_\theta : \mathcal{S} \rightarrow \mathcal{A}$$和一个方差参数$$\sigma$$来参数化随机策略$$\pi_{\mu_{\theta, \sigma}}$$，比如若$$\sigma = 0$$则随机策略可以等价于确定性策略$$\pi_{\mu_\theta, 0} \equiv \mu_\theta$$。然后，可以证明当$$\sigma \rightarrow 0$$的时候，随机策略梯度收敛到确定性梯度（证明和条件见附录C）。

<b><font color="#00B050">Theorem 2.</font></b> 考虑一个随机策略$$\pi_{\mu_{\theta, \sigma}}$$使得$$\pi_{\mu_{\theta, \sigma}}(a \vert s) = \nu_\sigma(\mu_\theta(s, a))$$，其中$$\sigma$$是控制方差的参数，并且$$\nu_\sigma$$满足条件B.1，MDP满足条件A.1和A.2。那么，

$$  \lim _{\sigma \downarrow 0} \nabla_\theta J\left(\pi_{\mu_\theta, \sigma}\right)
=   \nabla_\theta J\left(\mu_\theta\right)
\tag{10} $$

其中，等式左边的梯度是标准的随机策略梯度，等式右边的梯度是确定性策略梯度。

&emsp;&emsp;该结论很重要，因为它表明所有我们熟悉的策略梯度的机制都是可以应用到确定性策略梯度上的，如compatible function approximation (Sutton et al., 1999), natural gradients (Kakade, 2001), actor-critic (Bhatnagar et al., 2007), 或episodic/batch methods (Peters et al., 2005)。





## A. Regularity Conditions

**A.1：**$$p(s^{\prime} \vert s,a),\ \nabla_a p(s^{\prime} \vert s, a),\ \mu_\theta(s),\ \nabla_\theta \mu_\theta(s),\ r(s,a),\ \nabla_a r(s,a),\ p_1(s)$$对于所有的参数和变量$$s, a, s^{\prime}, x$$都是连续的。

<!-- 蓝 -->
<b><font color="#3399ff"></font></b>
<!-- 绿 --><!-- #33cc00 -->
<b><font color="#00B050"></font></b>
<!-- 橙 -->
<b><font color="#FF4500"></font></b>