---
sort: 4
---

# Deterministic Policy Gradient

Deterministic Policy Gradient Algorithms. [C]//International conference on machine learning (ICML). Silver D, Lever G, Heess N, et al. , 2014

<br/>

*摘要*——本文针对强化学习中的连续问题，提出了**确定性**策略梯度算法。确定性策略梯度具有非常吸引人的形式：它是动作-价值函数的期望梯度。这种简单的形式意味着确定性策略梯度可以比一般的随机策略梯度更有效地估计。为保证充分的探索，本文提出了一种离线的actor-critic算法，该算法从一个探索性的行为策略中学习确定性目标策略。在高维动作空间中，该算法的性能显著优于随机策略。

## 4.1 Introduction

&emsp;&emsp;策略梯度算法的基本思想是用一个参数化的策略分布$$\pi_\theta(a \vert s) = \mathbb{P}[a \vert s; \theta]$$来表示策略，处于状态$$s$$下时根据参数向量$$\theta$$随机选择动作$$a$$。策略梯度算法通常通过对这种随机梯度进行采样，并往累计奖励更大的方向调整策略参数。

&emsp;&emsp;与一般的策略梯度算法不同，本文采用确定性策略$$a = \mu_\theta(s)$$。与过去的结论不同，本文证明了确定性策略梯度确实存在，并且有简单的model-free的形式，即跟随action-value函数的梯度。此外，本文还证明了确定性策略梯度是随机策略梯度的一种特殊情况，即策略方差趋于0的情况。

&emsp;&emsp;从实践的角度讲，随机与确定策略梯度有非常关键的不同之处。在确定性的情况下，策略梯度只在状态空间上积分。

&emsp;&emsp;为了保证确定性策略算法能够持续探索，本文提出了一个off-policy的学习算法。其基本思想是根据一个随机行为策略来选择动作（保证充分的探索），但是学习一个确定的目标策略（利用确定性策略梯度的效率）。本文利用确定性策略梯度推导出一个off-policy actor-critic算法，该算法用一个可微的函数逼近器来估计动作价值函数，然后沿近似的动作价值函数的梯度方向来更新策略参数。此外，本文还引入了确定性策略梯度的相容函数逼近概念，以此保证对策略梯度的无偏估计。

&emsp;&emsp;实验结果表明，确定性策略梯度的性能明显优于随机策略梯度，尤其是在高维任务中。进一步地，本文提出的方法计算量并不比先前的方法大：每次更新的计算成本在动作维度和策略参数量上是线性的。最后，还有很多应用（如在机器人中）提供了可微的控制策略，但没有给控制器增加噪声的功能。在这些情况下，随机策略梯度是不适用的，而本文的方法可能仍然有用。





<!-- 蓝 -->
<b><font color="#3399ff"></font></b>
<!-- 绿 --><!-- #33cc00 -->
<b><font color="#00B050"></font></b>
<!-- 橙 -->
<b><font color="#FF4500"></font></b>