---
sort: 3
---

# Proximal Policy Optimization

[Proximal Policy Optimization Algorithms](http://arxiv.org/abs/1707.06347)John Schulman, F. Wolski, P. Dhariwal, A. Radford and O. Klimov, 2017

<br/>

*摘要*——文章提出了一类新的用于强化学习的策略梯度方法，在通过与环境交互采样数据时交替，并使用随机梯度上升优化一个“替代”目标函数。在标准策略梯度方法中每次采样都会进行一次梯度更新，本文提出了一种新的目标函数，使得在多个epochs用minibatch更新成为可能。新方法称为proximal policy optimization (PPO)，具有TRPO的部分优点，但更易于实现，更一般化，且有更好的采样复杂度（从经验上来说）。实验表明，PPO优于其它在线策略梯度方法，总体上在采样复杂度、简洁性和延迟时间之间取得了良好的平衡。

## 1 Introduction

&emsp;&emsp;利用神经网络作函数逼近器的强化学习中，主要的方法有deep Q-learning、原始的策略梯度方法和置信域/自然梯度方法。但是，在以下方面还有改进空间：可扩展性（拓展到大的模型和并行实现）、数据高效和鲁棒性（i.e. 无需调超参数即可胜任不同任务）。

&emsp;&emsp;本文试图介绍一种算法，该算法在只使用一阶优化的情况下，达到TRPO的数据效率和可靠性能。我们提出了一个新的截断概率比（clipped probability ratios）目标，形成了对策略表现的悲观估计（i.e. 下界）。为了优化策略，我们在从策略中采样数据和对采样数据进行多次优化之间进行交替。

&emsp;&emsp;实验对比了不同的替代目标，发现截断概率比目标最好。与其它方法相比，PPO在连续控制任务中表现得比其它方法好。在Atari中，（在样本复杂度上的）性能显著地比A2C好，且与ACER相当，但PPO更简单。

## 2 Background

### 2.1 Policy Gradient Methods

&emsp;&emsp;策略梯度方法计算策略梯度的估计量，并将其插入到随机梯度上升算法中。虽然提倡使用相同的轨迹对目标函数进行多步优化，但这样做的理由并不充分，而且在经验上往往会导致<b><font color="#FF4500">破坏性的大的策略更新</font></b>。

### 2.2 Trust Region Methods

&emsp;&emsp;TRPO中，在策略更新大小的约束下最大化目标函数（“替代”目标）。在论证TRPO时，实际使用的是惩罚项而不是约束条件。这源于某个替代目标（它计算状态上的最大KL而不是均值）对策略$$\pi$$的性能形成了一个下界。TRPO用硬约束来代替惩罚项是因为很难选择合适的罚系数$$\beta$$，固定的$$\beta$$无法胜任不同的问题，甚至在一个简单的问题上都可能效果不好。要想<b><font color="#FF4500">用一阶算法来模拟TRPO的单调改进很难</font></b>，选择一个固定的罚系数并用SGD来优化“替代”目标函数是不够的，还需要更多修饰。

## 3 Clipped Surrogate Objective

&emsp;&emsp;用$$r_t(\theta)$$表示概率比值$$r_t(\theta) = \frac{\pi_{\theta}(a_t \vert s_t)}{\pi_{\theta_{\text{old}}}(a_t \vert s_t)}$$，所以有$$r_t(\theta_{\text{old}}) = 1$$。TRPO最大化的“替代”目标即

$$  L^{\text{CPI}}(\theta)
=   \hat{\mathbb{E}}_t \left[
        \frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{\text {old }}}(a_t \mid s_t)} \hat{A}_t
    \right]
=   \hat{\mathbb{E}}_t\left[r_t(\theta) \hat{A}_t\right]
$$

上标CPI表示conservative policy iteration（保守策略迭代）。在没有约束条件的情况下，最大化这个目标函数会带来非常大的策略更新，所以下面我们需要修饰这个目标，惩罚那些让$$r_t(\theta)$$远离1的策略变化。

&emsp;&emsp;我们提出了如下目标函数：

$$  L^{\text{CLIP}}(\theta)
=   \hat{\mathbb{E}}_t \left[ 
        \min \left( r_t(\theta) \hat{A}_t, 
                    \operatorname{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t \right) 
    \right]
$$

```tip
clip( , , )函数，当第一项小于第二项时，函数值 = 第二项；当第一项大于第三项时，函数值 = 第三项；否则，函数值 = 第一项。
```

其中$$\epsilon$$是一个超参数，这里$$\epsilon = 0.2$$。$$\min$$函数中第一项即$$L^{\text{CPI}}$$。第二项$$\operatorname{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t$$通过截断概率比值的方式修饰了“替代”目标，移除了将$$r_t$$移出$$[1-\epsilon, 1+\epsilon]$$的激励。通过取截断目标和未截断目标中更小的，就可以得到未截断目标的一个下界。通过这个机制，我们可以忽略让目标更好的概率比的变化，但是考虑让目标更差的概率比的变化。注意，在$$\theta_{\text{old}}$$附近$$L^{\text{CLIP}} = L^{\text{CPI}}$$一阶成立，但是$$\theta$$离开$$\theta_{\text{old}}$$后就不成立了。注意，概率比在$$1-\epsilon$$处还是$$1+\epsilon$$处截断取决于adavantage $$\hat{A}_t$$的正负。下图是$$L^{\text{CLIP}}$$中的一项（i.e. 某个时刻$$t$$），整个$$L^{\text{CLIP}}$$是很多这样的项的和。

<figure><img src="./images/PPO_1.JPG" width=500px></figure>

&emsp;&emsp;下图是“替代”目标$$L^{\text{CLIP}}$$另一种直觉来源，展示了在一个连续的控制问题上通过近端策略优化（我们将简要介绍的算法）得到的沿策略更新方向插值时，几个目标是如何变化的。

<figure>
    <img src="./images/PPO_2.JPG" width=600px>
    <figcaption>“替代”目标，因为我们在初始策略参数和更新的策略参数之间进行插值，而更新的策略参数是在PPO迭代一次之后计算的。更新后的策略与初始策略的KL散度约为0.02，此时L<sup>CLIP</sup>最大。该图对应于Hopper-v1问题上的第一次策略更新，使用了6.1节中提供的超参数。<figcaption>
</figure>





<!-- 蓝 -->
<b><font color="#3399ff"></font></b>
<!-- 绿 --><!-- #33cc00 -->
<b><font color="#00B050"></font></b>
<!-- 橙 -->
<b><font color="#FF4500"></font></b>