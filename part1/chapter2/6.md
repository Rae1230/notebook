---
sort: 6
---

# Twin Delayed DDPG (TD3)

Addressing Function Approximation Error in Actor-Critic Methods. [C]//PMLR. Scott Fujimoto, Herke van Hoof, David Meger. 2018

<br/>

*摘要*——在基于价值的强化学习方法中，如deep Q-learning，函数逼近器的误差会导致高估价值估计和次优策略。本文证明了这个问题在actor-critic架构下仍然存在，并提出了新的机制来最小化它对actor和critic的影响。TD3是基于Double Q-learning的，通过取两个critic中较小的来防止高估。本文描述了目标网络和高估偏差之间的关系，并提出用延迟策略更新来减少每次更新的误差，从而进一步提高性能。利用OpenAI gym进行测试，TD3在每个环境中都表现出了SOTA的性能。

# 1 Introduction

&emsp;&emsp;本文将指出TD方法中的高估偏差和累计误差在actor-critic架构中仍然存在。

&emsp;&emsp;高估偏差偏差是Q-learning的性质，对包含噪声的价值估计做最大化会带来持续存在的overestimation。在使用函数逼近器的情况下，由于估计误差的存在噪声是不可避免的。这种不准确性会被TD学习的特性进一步放大。这意味着每步预测都用了不准确的估计，这将带来累计误差。因为高估误差的存在，这种累计误差可能导致任意不好的状态被估计成一个很高的价值，导致次优的策略更新和发散的行为。

&emsp;&emsp;本文首先证明在连续控制的情况下，高估现象在确定性策略梯度的情况下也是存在的。进一步指出离散动作情况的普适解，Double DQN在actor-critic架构下无法使用。在训练过程中，Double DQN用一个单独的目标价值函数来估计当前策略的价值，使得动作评估的过程中没有最大化偏差。然而，由于actor-critic架构中策略改变较慢，当前与目标价值估计保持十分相似的关系，防止最大化偏差的存在。这个问题可以通过使用一个更早的变体Double Q-learning来解决，通过独立地训练一组critic来**将Double Q-learning拓展到actor-critic架构下**。尽管这样价值估计的偏差会减小，甚至是无偏的，但估计仍有很大的方差，仍然会导致在状态空间的局部存在高估。针对这个问题，本文提出了一个**clipped Double Q-learning**变体，将存在高估偏差的价值估计作为对真值估计的近似上界。这会有利于低估，而低估在学习过程中并不会被传播，因为低价值估计的动作会被策略规避掉。

&emsp;&emsp;考虑到噪声与高估偏差之间的关系，本文包含了多个减小方差的方法。首先，本文证明了目标网络对于减小方差有很重要的作用，因为它们能防止误差累计。然后，为了解决价值与策略的耦合问题，本文提出了在价值估计收敛前延迟策略更新。最后，本文提出了一个新的正则化方法，通过SARSA风格的更新bootstrap相似的动作估计来进一步减小方差。

&emsp;&emsp;本文在DDPG的基础上进行改进，得到Twin Delayed Deep Deterministic policy gradient algorithm (TD3)，TD3是actor-critic架构的，考虑了函数逼近误差与策略和价值更新之间的关系。

# 3 Background

&emsp;&emsp;强化学习的目标就是找到最优策略$$\pi_{\phi}$$，最大化期望return $$J(\theta) = \mathbb{s_i \sim p_{\pi}, a_i \sim \pi} [R_0]$$。在actor-critic方法中，策略即为actor，可以通过确定性策略梯度算法来更新：

$$  \nabla_\phi J(\phi)
=   \mathbb{E}_{s \sim p_\pi} \left[\left.\nabla_a Q^\pi(s, a) \right|_{a=\pi(s)} \nabla_\phi \pi_\phi(s)\right]
\tag{1} $$

其中，$$Q^\pi(s, a) = \mathbb{E}_{s_i \sim p_\pi, a_i \sim \pi}\left[R_t \mid s, a\right]$$即为critic。

&emsp;&emsp;在Q-learning中，$$Q$$函数可以通过TD的方式学习，基于式(2) 的Bellman方程进行更新。

$$  Q^\pi(s, a)
=   r + \gamma \mathbb{E}_{s^{\prime}, a^{\prime}}[Q^\pi(s^{\prime}, a^{\prime})], \quad a^{\prime} \sim \pi(s^{\prime})
\tag{2} $$

对于大的状态空间，可以用一个可微的函数逼近器$$Q_{\theta}(s, a)$$来估计$$Q$$。在deep Q-learning中，用TD更新网络的同时，还用到了一个固定目标网络$$Q_{\theta^{\prime}}(s, a)$$在多次更新的过程中保持一个固定的目标$$y$$：

$$ y = r + \gamma Q_{\theta^{\prime}}(s^{\prime}, a^{\prime}), \quad a^{\prime} \sim \pi_{\phi^{\prime}}(s^{\prime}) \tag{3} $$

其中，动作是根据目标actor网络$$\pi_{\phi^{\prime}}$$选择的。目标网络的权重可以周期性地直接把当前网络的权重直接完全拷贝过来，或者在每个时间步以一定比例更新，$$\theta^{\prime} \leftarrow \tau \theta + (1 - \tau)\theta^{\prime}$$。这种更新可以用于off-policy学习，从经验回放池进行随机minibatch采样。

# 4 Overestimation Bias

&emsp;&emsp;在离散动作的情况下，高估偏差明显是最大化造成的伪迹。而在actor-critic架构下，高估偏差的存在和影响相对来说不那么明显，因为策略是通过梯度下降的方式学习的。首先证明在4.1节的一些基本假设下，确定性策略梯度中的价值估计将是高估的。然后在4.2节中，本文将提出Double Q-learning在actor-critic架构下的一个clipped变体，减小高估偏差。

## 4.1 Overestimation Bias in Actor-Critic

&emsp;&emsp;本节中假设策略是根据确定性策略梯度更新的，并证明这种更新方式会导致高估。给定当前策略参数$$\phi$$，用$$\phi_{\text{approx}}$$表示根据近似critic $$Q_{\theta}(s,a)$$





<!-- 蓝 -->
<b><font color="#3399ff"></font></b>
<!-- 绿 --><!-- #33cc00 -->
<b><font color="#00B050"></font></b>
<!-- 橙 -->
<b><font color="#FF4500"></font></b>