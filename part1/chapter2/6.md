---
sort: 6
---

# Twin Delayed DDPG (TD3)

Addressing Function Approximation Error in Actor-Critic Methods. [C]//PMLR. Scott Fujimoto, Herke van Hoof, David Meger. 2018

<br/>

*摘要*——在基于价值的强化学习方法中，如deep Q-learning，函数逼近器的误差会导致高估价值估计和次优策略。本文证明了这个问题在actor-critic架构下仍然存在，并提出了新的机制来最小化它对actor和critic的影响。TD3是基于Double Q-learning的，通过取两个critic中较小的来防止高估。本文描述了目标网络和高估偏差之间的关系，并提出用延迟策略更新来减少每次更新的误差，从而进一步提高性能。利用OpenAI gym进行测试，TD3在每个环境中都表现出了SOTA的性能。

# 1 Introduction

&emsp;&emsp;本文将指出TD方法中的高估偏差和累计误差在actor-critic架构中仍然存在。

&emsp;&emsp;高估偏差偏差是Q-learning的性质，对包含噪声的价值估计做最大化会带来持续存在的overestimation。在使用函数逼近器的情况下，由于估计误差的存在噪声是不可避免的。这种不准确性会被TD学习的特性进一步放大。这意味着每步预测都用了不准确的估计，这将带来累计误差。因为高估误差的存在，这种累计误差可能导致任意不好的状态被估计成一个很高的价值，导致次优的策略更新和发散的行为。

&emsp;&emsp;本文首先证明在连续控制的情况下，高估现象在确定性策略梯度的情况下也是存在的。进一步指出离散动作情况的普适解，Double DQN在actor-critic架构下无法使用。在训练过程中，Double DQN用一个单独的目标价值函数来估计当前策略的价值，使得动作评估的过程中没有最大化偏差。然而，由于actor-critic架构中策略改变较慢，当前与目标价值估计保持十分相似的关系，防止最大化偏差的存在。这个问题可以通过使用一个更早的变体Double Q-learning来解决，通过独立地训练一组critic来**将Double Q-learning拓展到actor-critic架构下**。尽管这样价值估计的偏差会减小，甚至是无偏的，但估计仍有很大的方差，仍然会导致在状态空间的局部存在高估。针对这个问题，本文提出了一个**clipped Double Q-learning**变体，将存在高估偏差的价值估计作为对真值估计的近似上界。这会有利于低估，而低估在学习过程中并不会被传播，因为低价值估计的动作会被策略规避掉。

&emsp;&emsp;考虑到噪声与高估偏差之间的关系，本文包含了多个减小方差的方法。首先，本文证明了目标网络对于减小方差有很重要的作用，因为它们能防止误差累计。然后，为了解决价值与策略的耦合问题，本文提出了在价值估计收敛前延迟策略更新。最后，本文提出了一个新的正则化方法，通过SARSA风格的更新bootstrap相似的动作估计来进一步减小方差。

&emsp;&emsp;本文在DDPG的基础上进行改进，得到Twin Delayed Deep Deterministic policy gradient algorithm (TD3)，TD3是actor-critic架构的，考虑了函数逼近误差与策略和价值更新之间的关系。

# 3 Background

&emsp;&emsp;强化学习的目标就是找到最优策略$$\pi_{\phi}$$，最大化期望return $$J(\theta) = \mathbb{s_i \sim p_{\pi}, a_i \sim \pi} [R_0]$$。在actor-critic方法中，策略即为actor，可以通过确定性策略梯度算法来更新：

$$  \nabla_\phi J(\phi)
=   \mathbb{E}_{s \sim p_\pi} \left[\left.\nabla_a Q^\pi(s, a) \right|_{a=\pi(s)} \nabla_\phi \pi_\phi(s)\right]
\tag{1} $$

其中，$$Q^\pi(s, a) = \mathbb{E}_{s_i \sim p_\pi, a_i \sim \pi}\left[R_t \mid s, a\right]$$即为critic。

&emsp;&emsp;在Q-learning中，$$Q$$函数可以通过TD的方式学习，基于式(2) 的Bellman方程进行更新。

$$  Q^\pi(s, a)
=   r + \gamma \mathbb{E}_{s^{\prime}, a^{\prime}}[Q^\pi(s^{\prime}, a^{\prime})], \quad a^{\prime} \sim \pi(s^{\prime})
\tag{2} $$

对于大的状态空间，可以用一个可微的函数逼近器$$Q_{\theta}(s, a)$$来估计$$Q$$。在deep Q-learning中，用TD更新网络的同时，还用到了一个固定目标网络$$Q_{\theta^{\prime}}(s, a)$$在多次更新的过程中保持一个固定的目标$$y$$：

$$ y = r + \gamma Q_{\theta^{\prime}}(s^{\prime}, a^{\prime}), \quad a^{\prime} \sim \pi_{\phi^{\prime}}(s^{\prime}) \tag{3} $$

其中，动作是根据目标actor网络$$\pi_{\phi^{\prime}}$$选择的。目标网络的权重可以周期性地直接把当前网络的权重直接完全拷贝过来，或者在每个时间步以一定比例更新，$$\theta^{\prime} \leftarrow \tau \theta + (1 - \tau)\theta^{\prime}$$。这种更新可以用于off-policy学习，从经验回放池进行随机minibatch采样。

# 4 Overestimation Bias

&emsp;&emsp;在离散动作的情况下，高估偏差明显是最大化造成的伪迹。而在actor-critic架构下，高估偏差的存在和影响相对来说不那么明显，因为策略是通过梯度下降的方式学习的。首先证明在4.1节的一些基本假设下，确定性策略梯度中的价值估计将是高估的。然后在4.2节中，本文将提出Double Q-learning在actor-critic架构下的一个clipped变体，减小高估偏差。

## 4.1 Overestimation Bias in AC

&emsp;&emsp;本节中假设策略是根据确定性策略梯度更新的，并证明这种更新方式会导致高估。给定当前策略参数$$\phi$$，用$$\phi_{\text{approx}}$$表示根据最大化近似critic $$Q_{\theta}(s,a)$$更新的actor参数，$$\phi_{\text{true}}$$表示根据真的价值函数$$Q^{\pi}(s,a)$$进行假设actor更新的参数（学习过程中是不知道的）：

$$\begin{aligned}
    \phi_{\text{approx}} 
& = \phi + \frac{\alpha}{Z_1} 
    \mathbb{E}_{s \sim p_\pi} \left[\left.\nabla_\phi \pi_\phi(s) \nabla_a Q_\theta(s, a) \right|_{a=\pi_\phi(s)}\right] \\
    \phi_{\text{true}} 
& = \phi + \frac{\alpha}{Z_2} 
    \mathbb{E}_{s \sim p_\pi} \left[\left.\nabla_\phi \pi_\phi(s) \nabla_a Q^\pi(s, a) \right|_{a=\pi_\phi(s)}\right]
\end{aligned}   \tag{4} $$

其中，假设$$Z_1$$和$$Z_2$$是用来对梯度进行正则化的，i.e.，使得$$Z^{-1} \| \mathbb{E}[\cdot] \| = 1$$。如果没有梯度正则化，那么高估偏差势必会出现在稍严格一点的条件下。补充材料中进一步研究了这个案例。用$$\pi_{\text{approx}}$$和$$\pi_{\text{true}}$$分别表示用$$\phi_{\text{approx}}$$和$$\phi_{\text{true}}$$参数化的策略。

&emsp;&emsp;由于梯度方向是局部最大化的，所以存在一个足够小的$$\epsilon_1$$使得如果$$\alpha \le \epsilon_1$$，那么$$\pi_{\text{approx}}$$的**近似值**的下界将由$$\pi_{\text{true}}$$的**近似值**决定：

$$  \mathbb{E}[Q_\theta(s, \pi_{\text{approx}}(s))] 
\ge \mathbb{E}[Q_\theta(s, \pi_{\text{true}}(s))]
\tag{5} $$

&emsp;&emsp;相反地，存在一个足够小的$$\epsilon_2$$使得如果$$\alpha \le \epsilon_2$$，那么$$\pi_{\text{approx}}$$的**真值**的上界将由$$\pi_{\text{true}}$$的**真值**决定：

$$  \mathbb{E}[Q^\pi(s, \pi_{\text{true}}(s))] 
\ge \mathbb{E}[Q^\pi(s, \pi_{\text{approx}}(s))]
\tag{6} $$

&emsp;&emsp;如果在期望中，价值估计至少和与$$\phi_{\text{true}}$$相关的真值一样大，$$\mathbb{E}[Q_\theta(s, \pi_{\text{true}}(s))] \ge \mathbb{E}[Q^\pi(s, \pi_{\text{true}}(s))]$$，那么式(5) 和(6) 意味着如果$$\alpha \le \min(\epsilon_1, \epsilon_2)$$，那么价值估计将会高估：

$$  \mathbb{E}[Q_\theta(s, \pi_{\text{approx}}(s))]
\ge \mathbb{E}[Q^\pi   (s, \pi_{\text{approx}}(s))]
\tag{7} $$

&emsp;&emsp;尽管这种高估在每次更新时都是最小的，但是这种误差的存在引起了两个问题。<b><font color="#FF4500">第一，如果不加控制，这种高估可能会在多步更新后发展成显著偏差。第二，不准确的价值估计可能会导致差的策略更新。</font></b>这个问题尤其严重，因为存在反馈回路，次优的critic可能会导致次优动作的出现频率变高，下次策略更新时会强化次优动作。

**这种理论上的高估在SOTA方法的实践中会出现吗？**  
<figure>
    <img src="./images/td3_1.JPG" width=400px>
    <figcaption>图1. DDPG与Cipped Double Q-learning中价值估计的高估偏差的对比，在MuJoCo环境中运行超过一百万个时间步长</figcaption>
</figure>

## 4.2 Clipped Double Q-Learning for AC

&emsp;&emsp;已有的减小高估偏差的方法在actor-critic架构下不起作用。本节提出了一种新的Double Q-learning的clipped变体，可以替换任意actor-critic方法中的critic。

&emsp;&emsp;在Double Q-learning中，通过维持两个分开的价值估计讲贪心更新与价值函数分离开，这两个价值估计更新时互相会用到对方。如果这两个价值估计是无关的，那么它们可以被用来对另一个价值估计选择的动作做无偏估计。在Double DQN中，作者提出用目标网络作为其中一个价值估计，并对当前价值网络求最大值得到贪心策略。Actor-critic架构下有类似的更新方式，在学习目标中用当前策略而不是目标策略：

$$  y = r + \gamma Q_{\theta^{\prime}}(s^{\prime}, \pi_\phi(s^{\prime}))    \tag{8} $$

&emsp;&emsp;但是，在实践过程中，actor-critic下策略更新较慢，当前和目标网络太相似无法做出独立的估计，也没有提供多少改进。反而，可以用原始的Double Q-learning的公式，一对actors $$(\pi_{\phi_1}, \pi_{\phi_2})$$和critics $$(Q_{\theta_1}, Q_{\theta_2})$$，其中$$\pi_{\phi_1}$$根据$$Q_{\theta_1}$$优化，$$\pi_{\phi_2}$$根据$$Q_{\theta_2}$$优化：

$$\begin{aligned}
    y_1 &= r + \gamma Q_{\theta^{\prime}_2}(s^{\prime}, \pi_{\phi_1}(s^{\prime}))   \\
    y_2 &= r + \gamma Q_{\theta^{\prime}_1}(s^{\prime}, \pi_{\phi_2}(s^{\prime}))
\end{aligned}   \tag{9} $$

<figure>
    <img src="./images/td3_2.JPG" width=400px>
    <figcaption>Double DQN的actor-critic变体 (DDQN-AC) 与Double Q-learning (DQ-AC) 中价值估计的高估偏差的对比，在MuJoCo环境中运行超过一百万个时间步长</figcaption>
</figure>

&emsp;&emsp;从图2 中可以看出DDQN-AC也和DDPG一样有类似的高估问题，而Double Q-learning的估计虽然有效得多，但也没有完全消除高估。6.1节进一步证明了这种减小在实验中是不够的。

&emsp;&emsp;由于$$\pi_{\phi_1}$$是根据$$Q_{\theta_1}$$优化的，在$$Q_{\theta_1}$$的目标更新中用一个独立的估计将防止策略更新引入偏差。但是，critics之间并不是完全独立的，因为在学习目标的时候会用到另一个critic和同一个经验回放池。所以，对于某些状态$$s$$可能会有$$Q_{\theta_2}(s^, \pi_{\phi_1}(s)) \gt Q_{\theta_1}(s, \pi_{\phi_1}(s))$$。这可能会导致$$Q_{\theta_1}(s, \pi_{\phi_1}(s))$$总体上高估了真值，并且在状态空间的特定区域高估还会被进一步放大。为了解决这个问题，本文提出直接用有偏估计$$Q_{\theta_1}$$来作为偏差较小的价值估计$$Q_{\theta_2}$$的上界。即取两个估计中较小的那个，<b><font color="#00B050">Cipped Double Q-learning算法的更新目标为：</font></b>

$$\color{green}{
    y_1 = r + \gamma \min_{i=1,2} Q_{\theta^{\prime}_i}(s^{\prime}, \pi_{\phi_1}(s^{\prime}))
}\tag{10} $$

&emsp;&emsp;采样Cipped Double Q-learning算法，价值目标不会给使用标准Q-learning的目标带来额外的高估。尽管这个更新策略可能会带来低估的问题，但显然高估偏差更需要避免，被低估的动作在策略更新的过程中不会被显式传播。

&emsp;&emsp;在实现中，通过只用一个根据$$Q_{\theta_1}$$更新的actor可以减小计算消耗。对于$$Q_{\theta_2}$$可以用同样的目标$$y_2 = y_1$$。如果$$Q_{\theta_2} \gt Q_{\theta_1}$$，那么更新等价于标准更新且无额外偏差。如果$$Q_{\theta_2} \lt Q_{\theta_1}$$，那么意味着出现了高估，价值减小的方式与Double Q-learning类似。有限MDP情况下的收敛性证明是从这个直觉出发的，详见补充材料。

&emsp;&emsp;还有一个次要好处，通过把函数逼近器的误差看作随机变量，我们可以看出取最小的操作会为具有更低方差估计误差的状态提供更高的价值，因为一组随机变量的最小值随随机变量方差的增大而减小。这意味着式(10) 的<b><font color="#3399ff">取最小值的操作会偏好具有低方差的价值估计的状态，稳定的学习目标会带来更安全的策略更新</font></b>。






<!-- 蓝 -->
<b><font color="#3399ff"></font></b>
<!-- 绿 --><!-- #33cc00 -->
<b><font color="#00B050"></font></b>
<!-- 橙 -->
<b><font color="#FF4500"></font></b>