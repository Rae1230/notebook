---
sort: 2
---

# MLP与优化算法

<figure><img src="./images/2-1.JPG" width=750px></figure>

<figure><img src="./images/2-10.JPG" width=550px></figure>

## 2.1 SGD家族

<figure><img src="./images/2-2.JPG" width=800px></figure>

<figure><img src="./images/2-3.JPG" width=700px></figure>

<figure><img src="./images/2-4.JPG" width=600px></figure>

<b><font color="#FF4500">缺点：梯度方向：收敛速度慢，可能在鞍点处震荡；  
&emsp;&emsp;&emsp;学习率：需要手动设定，非最优。</font></b>

## 2.2 SGD with Momenturn

<figure><img src="./images/2-5.JPG" width=800px></figure>

## 2.3 SGD with NAG

<figure><img src="./images/2-6.JPG" width=600px></figure>

## 2.4 AdaGrad

<figure><img src="./images/2-7.JPG" width=600px></figure>

## 2.5 RMSprop

<figure><img src="./images/2-8.JPG" width=600px></figure>

## 2.6 Adam & Nadam

<figure><img src="./images/2-9.JPG" width=660px></figure>

思考：[Adam那么棒，为什么还对SGD念念不忘？](https://www.zhihu.com/column/juliuszh)

## 2.7 梯度折断

&emsp;&emsp;一种比较简单的启发式方法，把梯度的模限定在一个区间，当梯度的模小于或大于这个区间时就进行截断。（在训练Transformer时经常用到）

$$\mathbf{g}_t = \max(\min(\mathbf{g}_t, b), a)$$

<figure><img src="./images/2-11.JPG" width=250px></figure>